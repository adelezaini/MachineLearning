{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "920e779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random, seed\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def FrankeFunction(x,y):\n",
    "    \"\"\"Evaluate the Franke Function: a two-variables function to create the dataset of vanilla problems\"\"\"\n",
    "    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))\n",
    "    term2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))\n",
    "    term3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))\n",
    "    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)\n",
    "    return term1 + term2 + term3 + term4\n",
    " \n",
    "def Plot_FrankeFunction(x,y,z, title=\"Dataset\"):\n",
    "    \"\"\"3D plot, suitable for plotting the Franke Function\"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 7))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    #ax = fig.gca(projection=\"3d\")\n",
    "\n",
    "    # Plot the surface.\n",
    "    surf = ax.plot_surface(x, y, z, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "\n",
    "    # Customize the z axis.\n",
    "    ax.set_zlim(-0.10, 1.40)\n",
    "    ax.set_xlabel(r\"$x$\")\n",
    "    ax.set_ylabel(r\"$y$\")\n",
    "    ax.set_zlabel(r\"$z$\")\n",
    "    ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "    # Add a color bar which maps values to colors.\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def create_xyz_dataset(n, mu, sigma):\n",
    "    \"\"\" Create xyz dataset from the FrankeFunction with a added normal distributed noise.\n",
    "    x,y variables are taken evenly distributed in the interval [0,1]\n",
    "    \n",
    "    Args:\n",
    "    n (int): squared root of total number of datapoints\n",
    "    mu (float): mean value of the normal distribution of the noise\n",
    "    sigma (float): standard deviation of the normal distribution of the noise\n",
    "\n",
    "    Returns x,y,z values, mashed on a grid.\n",
    "    \"\"\"\n",
    "    x = np.linspace(0,1,n)\n",
    "    y = np.linspace(0,1,n)\n",
    "\n",
    "    x,y = np.meshgrid(x,y)\n",
    "    z = FrankeFunction(x,y) + mu + sigma * np.random.randn(n,n)\n",
    "    \n",
    "    return x,y,z\n",
    "\n",
    "def create_X(x, y, n):\n",
    "    \"\"\"Design matrix for two indipendent variables x,y\"\"\"\n",
    "    if len(x.shape) > 1:\n",
    "        x = np.ravel(x)\n",
    "        y = np.ravel(y)\n",
    "\n",
    "    N = len(x)\n",
    "    l = int((n+1)*(n+2)/2)\t\t# Number of elements in beta, number of feutures (degree of polynomial)\n",
    "    X = np.ones((N,l))\n",
    "\n",
    "    for i in range(1,n+1):\n",
    "        q = int((i)*(i+1)/2)\n",
    "        for k in range(i+1):\n",
    "            X[:,q+k] = (x**(i-k))*(y**k)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "39c9f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MIT License (MIT)\n",
    "#\n",
    "# Copyright © 2021 Adele Zaini\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n",
    "# documentation files (the “Software”), to deal in the Software without restriction, including without limitation the\n",
    "# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,\n",
    "# and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all copies or substantial portions of\n",
    "# the Software. THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT\n",
    "# LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n",
    "# SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF\n",
    "# CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n",
    "# IN THE SOFTWARE.\n",
    "\n",
    "import numpy as np\n",
    "from random import random, seed\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Error analysis\n",
    "def R2(z_data, z_model):\n",
    "    \"\"\"Compute the R2 score of the two given values\"\"\"\n",
    "    return 1 - np.sum((z_data - z_model) ** 2) / np.sum((z_data - np.mean(z_data)) ** 2)\n",
    "def MSE(z_data,z_model):\n",
    "    \"\"\"Compute the Mean Square Error of the two given values\"\"\"\n",
    "    n = np.size(z_model)\n",
    "    return np.sum((z_data-z_model)**2)/n\n",
    "\n",
    "def SVD(A):\n",
    "    \"\"\" Application of SVD theorem.\n",
    "    Useful for debugging. \"\"\"\n",
    "    U, S, VT = np.linalg.svd(A,full_matrices=True)\n",
    "    D = np.zeros((len(U),len(VT)))\n",
    "    print(\"shape D= \", np.shape(D))\n",
    "    print(\"Shape S= \",np.shape(S))\n",
    "    print(\"lenVT =\",len(VT))\n",
    "    print(\"lenU =\",len(U))\n",
    "    D = np.eye(len(U),len(VT))*S\n",
    "    \"\"\"\n",
    "    for i in range(0,VT.shape[0]): #was len(VT)\n",
    "        D[i,i]=S[i]\n",
    "        print(\"i=\",i)\"\"\"\n",
    "    return U @ D @ VT\n",
    "    \n",
    "def SVDinv(A):\n",
    "    \"\"\"Evaluate the inverse of a matrix using the SVD theorem\"\"\"\n",
    "    U, s, VT = np.linalg.svd(A)\n",
    "    # reciprocals of singular values of s\n",
    "    d = 1.0 / s\n",
    "    # create m x n D matrix\n",
    "    D = np.zeros(A.shape)\n",
    "    # populate D with n x n diagonal matrix\n",
    "    D[:A.shape[1], :A.shape[1]] = np.diag(d)\n",
    "    UT = np.transpose(U)\n",
    "    V = np.transpose(VT)\n",
    "    return np.matmul(V,np.matmul(D.T,UT))\n",
    "    \n",
    "    \n",
    "class LinearRegression:\n",
    "    \"\"\"A class that gathers OLS, Ridge, Lasso methods\n",
    "\n",
    "    The 'fit' method needs to be implemented.\"\"\"\n",
    "\n",
    "    def __init__(self, X, z):\n",
    "        self.X = X\n",
    "        self.z = z\n",
    "        \n",
    "    def split(self,test_size=0.2):\n",
    "        self.X_train, self.X_test, self.z_train, self.z_test = train_test_split(self.X, self.z, test_size=test_size)\n",
    "        return self.X_train, self.X_test, self.z_train, self.z_test\n",
    "            \n",
    "    def rescale(self, with_std=False): #Improvement: pass the Scaler\n",
    "        \"\"\" z needs to be raveled \"\"\"\n",
    "        scaler_X = StandardScaler(with_std=with_std)\n",
    "        scaler_X.fit(self.X_train)\n",
    "        self.X_train = scaler_X.transform(self.X_train)\n",
    "        self.X_test = scaler_X.transform(self.X_test)\n",
    "\n",
    "        scaler_z = StandardScaler(with_std=with_std)\n",
    "        self.z_train = np.squeeze(scaler_z.fit_transform(self.z_train.reshape(-1, 1))) #scaler_z.fit_transform(z_train) #\n",
    "        self.z_test = np.squeeze(scaler_z.transform(self.z_test.reshape(-1, 1))) #scaler_z.transform(z_test) #\n",
    "        \n",
    "        return self.X_train, self.X_test, self.z_train, self.z_test\n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"Fit the model and return beta-values\"\"\"\n",
    "        raise NotImplementedError(\"Method LinearRegression.fit is abstract and cannot be called\")\n",
    "        \n",
    "    def fitSVD(self):\n",
    "        \"\"\"Fit the model and return beta-values, using SVD theorem to evalute the inverse of the matrix\"\"\"\n",
    "        raise NotImplementedError(\"Method LinearRegression.fitSVD is abstract and cannot be called\")\n",
    "    \n",
    "    def predict_train(self):\n",
    "        return self.X_train @ self.beta\n",
    "        \n",
    "    def predict_test(self):\n",
    "        return self.X_test @ self.beta\n",
    "    \n",
    "    def MSE_train(self, prec=4):\n",
    "        return np.round(MSE(self.z_train,self.predict_train()),prec)\n",
    "        \n",
    "    def MSE_test(self, prec=4):\n",
    "        return np.round(MSE(self.z_test,self.predict_test()),prec)\n",
    "        \n",
    "    def R2_train(self, prec=4):\n",
    "        return np.round(R2(self.z_train,self.predict_train()),prec)\n",
    "        \n",
    "    def R2_test(self, prec=4):\n",
    "        return np.round(R2(self.z_test,self.predict_test()),prec)\n",
    "        \n",
    "    def Confidence_Interval(self, sigma=1):\n",
    "        #Calculates variance of beta, extracting just the diagonal elements of the matrix\n",
    "        #var(B_j)=sigma^2*(X^T*X)^{-1}_{jj}\n",
    "        beta_variance = np.diag(sigma**2 * np.linalg.pinv(self.X.T @ self.X))\n",
    "        ci1 = self.beta - 1.96 * np.sqrt(beta_variance)/(self.X.shape[0])\n",
    "        ci2 = self.beta + 1.96 * np.sqrt(beta_variance)/(self.X.shape[0])\n",
    "        print('Confidence interval of β-estimator at 95 %:')\n",
    "        ci_df = {r'$β_{-}$': ci1,\n",
    "                 r'$β_{ols}$': self.beta,\n",
    "                 r'$β_{+}$': ci2}\n",
    "        ci_df = pd.DataFrame(ci_df)\n",
    "        display(np.round(ci_df,3))\n",
    "        return ci1, ci2\n",
    "        \n",
    "\"\"\"\n",
    "    def predict(self, X):\n",
    "        Fit the model and return the prediction\n",
    "        \n",
    "        Args:\n",
    "        X (array): design matrix\n",
    "\n",
    "        Returns X*beta\n",
    "        \n",
    "        raise NotImplementedError(\"Method LinearRegression.predict is abstract and cannot be called\")\n",
    "\"\"\"\n",
    "\n",
    "class OLSRegression(LinearRegression):\n",
    "    \n",
    "    def __init__(self, X, z):\n",
    "        super().__init__(X, z)\n",
    "        \n",
    "    def split(self, test_size=0.2):\n",
    "        return super().split(test_size=test_size)\n",
    "        \n",
    "    def rescale(self, with_std=False):\n",
    "        return super().rescale(with_std=with_std)\n",
    "        \n",
    "    def fit(self):\n",
    "        self.beta = np.linalg.pinv(self.X_train.T @ self.X_train) @ self.X_train.T @ self.z_train\n",
    "        return self.beta\n",
    "        \n",
    "    def fitSVD(self):\n",
    "        self.beta = SVDinv(self.X_train.T @ self.X_train) @ self.X_train.T @ self.z_train\n",
    "        return self.beta\n",
    "          \n",
    "    def predict_train(self):\n",
    "        return super().predict_train()\n",
    "        \n",
    "    def predict_test(self):\n",
    "        return super().predict_test()\n",
    "        \n",
    "    def MSE_train(self, prec=4):\n",
    "        return super().MSE_train(prec=prec)\n",
    "        \n",
    "    def MSE_test(self, prec=4):\n",
    "        return super().MSE_test(prec=prec)\n",
    "        \n",
    "    def R2_train(self, prec=4):\n",
    "        return super().R2_train(prec=prec)\n",
    "        \n",
    "    def R2_test(self, prec=4):\n",
    "        return super().R2_test(prec=prec)\n",
    "        \n",
    "    def Confidence_Interval(self, sigma=1):\n",
    "        return super().Confidence_Interval(sigma=sigma)\n",
    "        \n",
    "class RidgeRegression(LinearRegression):\n",
    "    \n",
    "    def __init__(self, X, z, lmb = 1e-12):\n",
    "        super().__init__(X, z)\n",
    "        self.lmd = lmd\n",
    "        \n",
    "    def split(self, test_size=0.2):\n",
    "        return super().split(test_size=test_size)\n",
    "        \n",
    "    def rescale(self, with_std=False):\n",
    "        return super().rescale(with_std=with_std)\n",
    "    \n",
    "    def fit(self):\n",
    "        self.beta = np.linalg.pinv(self.X_train.T @ self.X_train + self.lmd * np.eye(len(self.X_train.T))) @ self.X_train.T @ self.z_train\n",
    "        return self.beta\n",
    "        \n",
    "    def fitSVD(self):\n",
    "        self.beta = SVDinv(self.X_train.T @ self.X_train + self.lmd * np.eye(len(self.X_train.T))) @ self.X_train.T @ self.z_train\n",
    "        return self.beta\n",
    "          \n",
    "    def predict_train():\n",
    "        return super().predict_train()\n",
    "        \n",
    "    def predict_test(self):\n",
    "        return super().predict_test()\n",
    "\n",
    "    def MSE_train(self, prec=4):\n",
    "        return super().MSE_train(prec=prec)\n",
    "        \n",
    "    def MSE_test(self, prec=4):\n",
    "        return super().MSE_test(prec=prec)\n",
    "        \n",
    "    def R2_train(self, prec=4):\n",
    "        return super().R2_train(prec=prec)\n",
    "        \n",
    "    def R2_test(self, prec=4):\n",
    "        return super().R2_test(prec=prec)\n",
    "        \n",
    "    def Confidence_Interval(self, sigma=1):\n",
    "        return super().Confidence_Interval(sigma=sigma)\n",
    "\n",
    "class LassoRegression(LinearRegression):\n",
    "\n",
    "    def __init__(self, X, z, lmb = 1e-12):\n",
    "        super().__init__(X, z)\n",
    "        self.lmd = lmd\n",
    "        \n",
    "    def split(self, test_size=0.2):\n",
    "        return super().split(test_size=test_size)\n",
    "        \n",
    "    def rescale(self, with_std=False):\n",
    "        return super().rescale(with_std=with_std)\n",
    "    \n",
    "    def fit(self):\n",
    "        RegLasso = linear_model.Lasso(self.lmd)\n",
    "        self.beta = RegLasso.fit(self.X_train,self.z_train)\n",
    "        return self.beta\n",
    "        \n",
    "    def fit(self):\n",
    "        return fit()\n",
    "          \n",
    "    def predict_train(self):\n",
    "        return super().predict_train()\n",
    "        \n",
    "    def predict_test(self):\n",
    "        return super().predict_test()\n",
    "        \n",
    "    def MSE_train(self, prec=4):\n",
    "        return super().MSE_train(prec=prec)\n",
    "        \n",
    "    def MSE_test(self, prec=4):\n",
    "        return super().MSE_test(prec=prec)\n",
    "        \n",
    "    def R2_train(self, prec=4):\n",
    "        return super().R2_train(prec=prec)\n",
    "        \n",
    "    def R2_test(self, prec=4):\n",
    "        return super().R2_test(prec=prec)\n",
    "        \n",
    "    def Confidence_Interval(self, sigma=1):\n",
    "        return super().Confidence_Interval(sigma=sigma)\n",
    "    \n",
    "\"\"\"\n",
    "def ols_reg(X_train, X_test, z_train, z_test):\n",
    "\n",
    "\t# Calculating Beta Ordinary Least Square Equation with matrix pseudoinverse\n",
    "    # Altervatively to Numpy pseudoinverse it is possible to use the SVD theorem to evalute the inverse of a matrix (even in case it is singular). Just replace 'np.linalg.pinv' with 'SVDinv'.\n",
    "\tols_beta = np.linalg.pinv(X_train.T @ X_train) @ X_train.T @ z_train\n",
    "\n",
    "\tz_tilde = X_train @ ols_beta # z_prediction of the train data\n",
    "\tz_predict = X_test @ ols_beta # z_prediction of the test data\n",
    "  \n",
    "\treturn ols_beta, z_tilde, z_predict\n",
    " \n",
    " def ridge_reg(X_train, X_test, z_train, z_test, lmd = 10**(-12)):\n",
    " \n",
    "    ridge_beta = np.linalg.pinv(X_train.T @ X_train + lmd*np.eye(len(X_train.T))) @ X_train.T @ z_train #psudoinverse\n",
    "    z_model = X_train @ ridge_beta #calculates model\n",
    "    z_predict = X_test @ ridge_beta\n",
    "\n",
    "    #finds the lambda that gave the best MSE\n",
    "    #best_lamda = lambdas[np.where(MSE_values == np.min(MSE_values))[0]]\n",
    "\n",
    "    return ridge_beta, z_model, z_predict\n",
    "    \n",
    "def lasso_reg(X_train, X_test, z_train, z_test, lmd = 10**(-12)):\n",
    "\n",
    "    RegLasso = linear_model.Lasso(lmd)\n",
    "    _ = RegLasso.fit(X_train,z_train)\n",
    "    z_model = RegLasso.predict(X_train)\n",
    "    z_predict = RegLasso.predict(X_test)\n",
    "\n",
    "    return z_model, z_predict\n",
    "\"\"\"\n",
    "\n",
    "# Return the rolling mean of a vector and two values at one sigma from the rolling average\n",
    "def Rolling_Mean(vector, windows=3):\n",
    "    vector_df = pd.DataFrame({'vector': vector})\n",
    "    # computing the rolling average\n",
    "    rolling_mean = vector_df.vector.rolling(windows).mean().to_numpy()\n",
    "    # computing the values at two sigmas from the rolling average\n",
    "    rolling_std = vector_df.vector.rolling(windows).std().to_numpy()\n",
    "    value_up = rolling_mean + rolling_std\n",
    "    value_down = rolling_mean - rolling_std\n",
    "    \n",
    "    return rolling_mean, value_down, value_up\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ea99b024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_Xz(X_train, X_test, z_train, z_test, with_std=False):\n",
    "    scaler_X = StandardScaler(with_std=with_std) #with_std=False\n",
    "    scaler_X.fit(X_train)\n",
    "    X_train = scaler_X.transform(X_train)\n",
    "    X_test = scaler_X.transform(X_test)\n",
    "\n",
    "    scaler_z = StandardScaler(with_std=with_std) #with_std=False\n",
    "    z_train = np.squeeze(scaler_z.fit_transform(z_train.reshape(-1, 1))) #scaler_z.fit_transform(z_train) #\n",
    "    z_test = np.squeeze(scaler_z.transform(z_test.reshape(-1, 1))) #scaler_z.transform(z_test) #  \n",
    "    return X_train, X_test, z_train, z_test\n",
    "\n",
    "# Splitting and rescaling data (rescaling is optional)\n",
    "# Default values: 20% of test data and the scaler is StandardScaler without std.dev.\n",
    "def Split_and_Scale(X,z,test_size=0.2, scale=True, with_std=False):\n",
    "\n",
    "    #Splitting training and test data\n",
    "    X_train, X_test, z_train, z_test = train_test_split(X, z, test_size=test_size)\n",
    "\n",
    "    # Rescaling X and z (optional)\n",
    "    if scale:\n",
    "        X_train, X_test, z_train, z_test = scale_Xz(X_train, X_test, z_train, z_test, with_std=with_std)\n",
    "      \n",
    "    return X_train, X_test, z_train, z_test\n",
    "\n",
    "# OLS equation\n",
    "def OLS_solver(X_train, X_test, z_train, z_test):\n",
    "\n",
    "    # Calculating Beta Ordinary Least Square Equation with matrix pseudoinverse\n",
    "    # Altervatively to Numpy pseudoinverse it is possible to use the SVD theorem to evalute the inverse of a matrix (even in case it is singular). Just replace 'np.linalg.pinv' with 'SVDinv'.\n",
    "    ols_beta = np.linalg.pinv(X_train.T @ X_train) @ X_train.T @ z_train\n",
    "\n",
    "    z_tilde = X_train @ ols_beta # z_prediction of the train data\n",
    "    z_predict = X_test @ ols_beta # z_prediction of the test data\n",
    "  \n",
    "    return ols_beta, z_tilde, z_predict\n",
    "    \n",
    "def ridge_reg(X_train, X_test, z_train, z_test, lmd = 10**(-12)):\n",
    " \n",
    "    ridge_beta = np.linalg.pinv(X_train.T @ X_train + lmd*np.eye(len(X_train.T))) @ X_train.T @ z_train #psudoinverse\n",
    "    z_model = X_train @ ridge_beta #calculates model\n",
    "    z_predict = X_test @ ridge_beta\n",
    "\n",
    "    #finds the lambda that gave the best MSE\n",
    "    #best_lamda = lambdas[np.where(MSE_values == np.min(MSE_values))[0]]\n",
    "\n",
    "    return ridge_beta, z_model, z_predict\n",
    "    \n",
    "def lasso_reg(X_train, X_test, z_train, z_test, lmd = 10**(-12)):\n",
    "\n",
    "    RegLasso = linear_model.Lasso(lmd)\n",
    "    _ = RegLasso.fit(X_train,z_train)\n",
    "    z_model = RegLasso.predict(X_train)\n",
    "    z_predict = RegLasso.predict(X_test)\n",
    "\n",
    "    return z_model, z_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "183de205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.08033333  0.21241667 -0.004625    0.16577083  0.16637153\n",
      "  -0.06782841  0.07257494  0.12575231  0.09994083 -0.10346354  0.01118949\n",
      "   0.05754982  0.08036039  0.04393807 -0.12024953 -0.02415488  0.01295761\n",
      "   0.03514108  0.04366974  0.00213328]]\n",
      "[[ 0.         -0.08633333  0.17075    -0.17129167  0.03035417  0.10907986\n",
      "  -0.19398582 -0.05271441  0.0182581   0.04084071 -0.18911169 -0.08118609\n",
      "  -0.03601934 -0.0034974  -0.01027019 -0.17523442 -0.08607818 -0.05448448\n",
      "  -0.03435209 -0.02087356 -0.04449469]]\n",
      "[-0.30980687  0.03407068]\n",
      "[-0.32305563  0.00526445]\n",
      "[  0.           6.0240481    2.77833014 -27.41271571 -11.59605851\n",
      "  -4.54493467  38.33903483  34.4407874   17.5722142  -16.00905112]\n",
      "[-0.29419307 -0.0832303  -0.25361917  0.30578607 -0.08940484  0.35642934\n",
      " -0.19681158  0.5963452  -0.15675562  0.23819927]\n",
      "[-0.2410186   0.03289853 -0.32184282 -0.29944618  0.41458793  0.23171393\n",
      "  0.10950287 -0.07146335  0.67845615 -0.33666404]\n",
      "––––––––––––––––––––––––––––––––––––––––––––\n",
      "Train MSE: 0.0113\n",
      "Test MSE: 0.0111\n",
      "––––––––––––––––––––––––––––––––––––––––––––\n",
      "Train R2: 0.8808\n",
      "Test R2: 0.8678\n",
      "––––––––––––––––––––––––––––––––––––––––––––\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "# Degree of the polynomial\n",
    "degree=5\n",
    "# Datapoints (squared root of datapoints -> meshgrid)\n",
    "n = 25\n",
    "# Paramaters of noise distribution\n",
    "mu = 0; sigma = 0.1\n",
    "# Parameter of splitting data\n",
    "test_size = 0.2\n",
    "\n",
    "x,y,z=create_xyz_dataset(n,mu,sigma)\n",
    "#Plot_FrankeFunction(x,y,z)\n",
    "z=z.ravel()\n",
    "X=create_X(x,y,degree)\n",
    "\n",
    "X_train, X_test, z_train, z_test = Split_and_Scale(X,np.ravel(z)) #StardardScaler, test_size=0.2, scale=true\n",
    "print(X_train[:1])\n",
    "print(X_test[:1])\n",
    "print(z_train[:2])\n",
    "print(z_test[:2])\n",
    "\n",
    "ols_beta, z_tilde,z_predict = OLS_solver(X_train, X_test, z_train, z_test)\n",
    "\n",
    "print(ols_beta[:10])\n",
    "print(z_tilde[:10])\n",
    "print(z_predict[:10])\n",
    "\n",
    "prec=4\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "print(\"Train MSE:\", np.round(MSE(z_train,z_tilde),prec))\n",
    "print(\"Test MSE:\", np.round(MSE(z_test,z_predict),prec))\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "print(\"Train R2:\", np.round(R2(z_train,z_tilde),prec))\n",
    "print(\"Test R2:\", np.round(R2(z_test,z_predict),prec))\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c1cf952f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.           5.9775372    2.62744824 -26.82754565 -11.60503448\n",
      "  -3.33370708  36.72760272  34.54406785  18.6218372  -20.17673741]\n",
      "[-0.08469855 -0.11421559  0.13643301 -0.16757094  0.65560671 -0.00895539\n",
      "  0.14397071  0.18081704  0.36333525 -0.12630596]\n",
      "[ 0.54809917  0.12061255 -0.32207662  0.36061367 -0.12275633 -0.07422399\n",
      "  0.09656214 -0.26153205 -0.10654021  0.2230573 ]\n",
      "––––––––––––––––––––––––––––––––––––––––––––\n",
      "Train MSE: 0.0108\n",
      "Test MSE: 0.0132\n",
      "––––––––––––––––––––––––––––––––––––––––––––\n",
      "Train R2: 0.8857\n",
      "Test R2: 0.8453\n",
      "––––––––––––––––––––––––––––––––––––––––––––\n",
      "––––––––––––––––––––––––––––––––––––––––––––\n",
      "Train MSE: 0.0108\n",
      "Test MSE: 0.0132\n",
      "––––––––––––––––––––––––––––––––––––––––––––\n",
      "Train R2: 0.8857\n",
      "Test R2: 0.8453\n",
      "––––––––––––––––––––––––––––––––––––––––––––\n",
      "Confidence interval of β-estimator at 95 %:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$β_{-}$</th>\n",
       "      <th>$β_{ols}$</th>\n",
       "      <th>$β_{+}$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.976</td>\n",
       "      <td>5.978</td>\n",
       "      <td>5.979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.626</td>\n",
       "      <td>2.627</td>\n",
       "      <td>2.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-26.837</td>\n",
       "      <td>-26.828</td>\n",
       "      <td>-26.818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-11.612</td>\n",
       "      <td>-11.605</td>\n",
       "      <td>-11.598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-3.343</td>\n",
       "      <td>-3.334</td>\n",
       "      <td>-3.324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>36.706</td>\n",
       "      <td>36.728</td>\n",
       "      <td>36.749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>34.528</td>\n",
       "      <td>34.544</td>\n",
       "      <td>34.560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18.606</td>\n",
       "      <td>18.622</td>\n",
       "      <td>18.638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-20.198</td>\n",
       "      <td>-20.177</td>\n",
       "      <td>-20.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-17.221</td>\n",
       "      <td>-17.198</td>\n",
       "      <td>-17.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-39.044</td>\n",
       "      <td>-39.026</td>\n",
       "      <td>-39.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-7.591</td>\n",
       "      <td>-7.575</td>\n",
       "      <td>-7.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-28.538</td>\n",
       "      <td>-28.521</td>\n",
       "      <td>-28.503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>41.339</td>\n",
       "      <td>41.361</td>\n",
       "      <td>41.384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.911</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>11.381</td>\n",
       "      <td>11.388</td>\n",
       "      <td>11.396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>11.422</td>\n",
       "      <td>11.429</td>\n",
       "      <td>11.437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-5.059</td>\n",
       "      <td>-5.052</td>\n",
       "      <td>-5.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15.988</td>\n",
       "      <td>15.995</td>\n",
       "      <td>16.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-20.776</td>\n",
       "      <td>-20.767</td>\n",
       "      <td>-20.758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    $β_{-}$  $β_{ols}$  $β_{+}$\n",
       "0    -0.000      0.000    0.000\n",
       "1     5.976      5.978    5.979\n",
       "2     2.626      2.627    2.629\n",
       "3   -26.837    -26.828  -26.818\n",
       "4   -11.612    -11.605  -11.598\n",
       "5    -3.343     -3.334   -3.324\n",
       "6    36.706     36.728   36.749\n",
       "7    34.528     34.544   34.560\n",
       "8    18.606     18.622   18.638\n",
       "9   -20.198    -20.177  -20.155\n",
       "10  -17.221    -17.198  -17.176\n",
       "11  -39.044    -39.026  -39.009\n",
       "12   -7.591     -7.575   -7.559\n",
       "13  -28.538    -28.521  -28.503\n",
       "14   41.339     41.361   41.384\n",
       "15    0.911      0.920    0.928\n",
       "16   11.381     11.388   11.396\n",
       "17   11.422     11.429   11.437\n",
       "18   -5.059     -5.052   -5.044\n",
       "19   15.988     15.995   16.003\n",
       "20  -20.776    -20.767  -20.758"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "––––––––––––––––––––––––––––––––––––––––––––\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "model=OLSRegression(X,z)\n",
    "X_train, X_test, z_train, z_test = model.split()\n",
    "X_train, X_test, z_train, z_test = model.rescale()\n",
    "#X_train, X_test, z_train, z_test = model.X_train, model.X_test, model.z_train, model.z_test #StardardScaler, test_size=0.2, scale=true\n",
    "\"\"\"print(X_train[:1])\n",
    "print(X_test[:1])\n",
    "print(z_train[:2])\n",
    "print(z_test[:2])\"\"\"\n",
    "\n",
    "beta = model.fit()\n",
    "print(beta[:10])\n",
    "z_tilde = model.predict_train()\n",
    "z_predict = model.predict_test()\n",
    "print(z_tilde[:10])\n",
    "print(z_predict[:10])\n",
    "\n",
    "prec=4\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "print(\"Train MSE:\", np.round(MSE(z_train,z_tilde),prec))\n",
    "print(\"Test MSE:\", np.round(MSE(z_test,z_predict),prec))\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "print(\"Train R2:\", np.round(R2(z_train,z_tilde),prec))\n",
    "print(\"Test R2:\", np.round(R2(z_test,z_predict),prec))\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "print(\"Train MSE:\", model.MSE_train())\n",
    "print(\"Test MSE:\", model.MSE_test())\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "print(\"Train R2:\", model.R2_train())\n",
    "print(\"Test R2:\", model.R2_test())\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "# Confidence interval\n",
    "beta1, beta2 = model.Confidence_Interval(0.1)\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5dcf14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
