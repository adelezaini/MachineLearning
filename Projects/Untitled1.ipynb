{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d8cc8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random, seed\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def FrankeFunction(x,y):\n",
    "    \"\"\"Evaluate the Franke Function: a two-variables function to create the dataset of vanilla problems\"\"\"\n",
    "    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))\n",
    "    term2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))\n",
    "    term3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))\n",
    "    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)\n",
    "    return term1 + term2 + term3 + term4\n",
    " \n",
    "def Plot_FrankeFunction(x,y,z, title=\"Dataset\"):\n",
    "    \"\"\"3D plot, suitable for plotting the Franke Function\"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 7))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    #ax = fig.gca(projection=\"3d\")\n",
    "\n",
    "    # Plot the surface.\n",
    "    surf = ax.plot_surface(x, y, z, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "\n",
    "    # Customize the z axis.\n",
    "    ax.set_zlim(-0.10, 1.40)\n",
    "    ax.set_xlabel(r\"$x$\")\n",
    "    ax.set_ylabel(r\"$y$\")\n",
    "    ax.set_zlabel(r\"$z$\")\n",
    "    ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "    # Add a color bar which maps values to colors.\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def create_xyz_dataset(n, mu, sigma):\n",
    "    \"\"\" Create xyz dataset from the FrankeFunction with a added normal distributed noise.\n",
    "    x,y variables are taken evenly distributed in the interval [0,1]\n",
    "    \n",
    "    Args:\n",
    "    n (int): squared root of total number of datapoints\n",
    "    mu (float): mean value of the normal distribution of the noise\n",
    "    sigma (float): standard deviation of the normal distribution of the noise\n",
    "\n",
    "    Returns x,y,z values, mashed on a grid.\n",
    "    \"\"\"\n",
    "    x = np.linspace(0,1,n)\n",
    "    y = np.linspace(0,1,n)\n",
    "\n",
    "    x,y = np.meshgrid(x,y)\n",
    "    z = FrankeFunction(x,y) + mu + sigma * np.random.randn(n,n)\n",
    "    \n",
    "    return x,y,z\n",
    "\n",
    "def create_X(x, y, n):\n",
    "    \"\"\"Design matrix for two indipendent variables x,y\"\"\"\n",
    "    if len(x.shape) > 1:\n",
    "        x = np.ravel(x)\n",
    "        y = np.ravel(y)\n",
    "\n",
    "    N = len(x)\n",
    "    l = int((n+1)*(n+2)/2)\t\t# Number of elements in beta, number of feutures (degree of polynomial)\n",
    "    X = np.ones((N,l))\n",
    "\n",
    "    for i in range(1,n+1):\n",
    "        q = int((i)*(i+1)/2)\n",
    "        for k in range(i+1):\n",
    "            X[:,q+k] = (x**(i-k))*(y**k)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48543c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MIT License (MIT)\n",
    "#\n",
    "# Copyright © 2021 Adele Zaini\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n",
    "# documentation files (the “Software”), to deal in the Software without restriction, including without limitation the\n",
    "# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,\n",
    "# and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all copies or substantial portions of\n",
    "# the Software. THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT\n",
    "# LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n",
    "# SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF\n",
    "# CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n",
    "# IN THE SOFTWARE.\n",
    "\n",
    "import numpy as np\n",
    "from random import random, seed\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "\n",
    "# Error analysis\n",
    "def R2(y_data, y_model):\n",
    "    \"\"\"Compute the R2 score of the two given values\"\"\"\n",
    "    return 1 - np.sum((y_data - y_model) ** 2) / np.sum((y_data - np.mean(y_data)) ** 2)\n",
    "def MSE(y_data,y_model):\n",
    "    \"\"\"Compute the Mean Square Error of the two given values\"\"\"\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n\n",
    "    \n",
    "def train_test_rescale(X_train, X_test, y_train, y_test, with_std=False):\n",
    "    \"\"\"Rescale train and test data using StandardScaler().\n",
    "    The standard deviation correction is not applied by default.\n",
    "    It's the analogous of train_test_split() by sklearn\"\"\"\n",
    "    \n",
    "    if len(y_train.shape) > 1:\n",
    "        print(\"You forgot to ravel your outputs! \\n Automatically ravelled in train_test_rescale()\")\n",
    "        y_train = np.ravel(y_train)\n",
    "        y_test = np.ravel(y_test)\n",
    "    \n",
    "    scaler_X = StandardScaler(with_std=with_std)\n",
    "    scaler_X.fit(X_train)\n",
    "    X_train = scaler_X.transform(X_train)\n",
    "    X_test = scaler_X.transform(X_test)\n",
    "\n",
    "    scaler_y = StandardScaler(with_std=with_std)\n",
    "    y_train = np.squeeze(scaler_y.fit_transform(y_train.reshape(-1, 1))) #scaler_y.fit_transform(y_train) #\n",
    "    y_test = np.squeeze(scaler_y.transform(y_test.reshape(-1, 1))) #scaler_y.transform(y_test) #\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def SVD(A):\n",
    "    \"\"\" Application of SVD theorem.\n",
    "    Useful for debugging. \"\"\"\n",
    "    U, S, VT = np.linalg.svd(A,full_matrices=True)\n",
    "    D = np.zeros((len(U),len(VT)))\n",
    "    print(\"shape D= \", np.shape(D))\n",
    "    print(\"Shape S= \",np.shape(S))\n",
    "    print(\"lenVT =\",len(VT))\n",
    "    print(\"lenU =\",len(U))\n",
    "    D = np.eye(len(U),len(VT))*S\n",
    "    \"\"\"\n",
    "    for i in range(0,VT.shape[0]): #was len(VT)\n",
    "        D[i,i]=S[i]\n",
    "        print(\"i=\",i)\"\"\"\n",
    "    return U @ D @ VT\n",
    "    \n",
    "def SVDinv(A):\n",
    "    \"\"\"Evaluate the inverse of a matrix using the SVD theorem\"\"\"\n",
    "    U, s, VT = np.linalg.svd(A)\n",
    "    # reciprocals of singular values of s\n",
    "    d = 1.0 / s\n",
    "    # create m x n D matrix\n",
    "    D = np.zeros(A.shape)\n",
    "    # populate D with n x n diagonal matrix\n",
    "    D[:A.shape[1], :A.shape[1]] = np.diag(d)\n",
    "    UT = np.transpose(U)\n",
    "    V = np.transpose(VT)\n",
    "    return np.matmul(V,np.matmul(D.T,UT))\n",
    "    \n",
    "    \n",
    "def GD(X, y, gradient, eta = 0.1, Niterations = 1000):\n",
    "    \"\"\"Gradient Descent Algorithm\n",
    "    \n",
    "        Args:\n",
    "        - X (array): design matrix (training data)\n",
    "        - y (array): output dataset (training data)\n",
    "        - gradient (function): function to compute the gradient\n",
    "        - eta (float): learning rate\n",
    "        - Niterations (int): number of iteration\n",
    "        \n",
    "        Returns:\n",
    "        beta/theta-values\"\"\"\n",
    "    theta = np.random.randn(X.shape[1],1)\n",
    "\n",
    "    for iter in range(Niterations):\n",
    "        gradients = grandient(X, y, theta) #2.0/X.shape[0] * X.T @ ((X @ beta) - y)\n",
    "        theta -= eta*gradients\n",
    "        \n",
    "    return theta\n",
    "    \n",
    "    \n",
    "def learning_schedule(t, t0=5, t1=50):\n",
    "    return t0/(t+t1)\n",
    "    \n",
    "def SGD(X,y, gradient, n_epochs, m, t0=5, t=50):\n",
    "    \"\"\"Stochastic Gradient Descent Algorithm\n",
    "    \n",
    "        Args:\n",
    "        - X (array): design matrix (training data)\n",
    "        - y (array): output dataset (training data)\n",
    "        - gradient (function): function to compute the gradient\n",
    "        - n_epochs (int): number of epochs\n",
    "        - m (int): number of minibatches\n",
    "        - t0 (float): initial paramenter to compute the learning rate\n",
    "        - t1 (float): sequential paramenter to compute the learning rate\n",
    "        \n",
    "        Returns:\n",
    "        beta/theta-values\"\"\"\n",
    "        \n",
    "    theta = np.random.randn(X.shape[1],1)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for i in range(m):\n",
    "            random_index = np.random.randint(m)\n",
    "            Xi = X[random_index:random_index+1]\n",
    "            yi = y[random_index:random_index+1]\n",
    "            gradients = gradient(Xi, yi, theta) * X.shape[0] #2.0 * Xi.T @ ((Xi @ theta)-yi)\n",
    "            eta = learning_schedule(epoch*m+i, t0=t0, t1=t1)\n",
    "            theta = theta - eta*gradients\n",
    "            \n",
    "    return theta\n",
    "    \n",
    "    \n",
    "class LinearRegression:\n",
    "    \"\"\"A class that gathers OLS, Ridge, Lasso methods\n",
    "\n",
    "    The 'fit' method needs to be implemented.\"\"\"\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def split(self,test_size=0.2):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=test_size)\n",
    "        return self\n",
    "            \n",
    "    def rescale(self, with_std=False): #Improvement: pass the Scaler\n",
    "        \"\"\" y needs to be raveled \"\"\"\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_rescale(self.X_train, self.X_test, self.y_train, self.y_test, with_std = with_std)\n",
    "        return self\n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"Fit the model and return beta-values\"\"\"\n",
    "        raise NotImplementedError(\"Method LinearRegression.fit is abstract and cannot be called\")\n",
    "        \n",
    "    def fitSVD(self):\n",
    "        \"\"\"Fit the model and return beta-values, using SVD theorem to evalute the inverse of the matrix\"\"\"\n",
    "        raise NotImplementedError(\"Method LinearRegression.fitSVD is abstract and cannot be called\")\n",
    "        \n",
    "    def fit_SK(self):\n",
    "        \"\"\"Fit the model and return beta-values, using Scikit-Learn\"\"\"\n",
    "        raise NotImplementedError(\"Method LinearRegression.fit_SK is abstract and cannot be called\")\n",
    "        \n",
    "    def fitGD(self):\n",
    "        \"\"\"Fit the model and return beta-values, using the Gradient Descent\"\"\"\n",
    "        raise NotImplementedError(\"Method LinearRegression.fitGD is abstract and cannot be called\")\n",
    "    \n",
    "    def fitSGD(self):\n",
    "        \"\"\"Fit the model and return beta-values, using the Stochastic Gradient Descent\"\"\"\n",
    "        raise NotImplementedError(\"Method LinearRegression.fitSGD is abstract and cannot be called\")\n",
    "        \n",
    "    def get_param(self):\n",
    "        return self.beta\n",
    "    \n",
    "    def predict_train(self):\n",
    "        return self.X_train @ self.beta\n",
    "        \n",
    "    def predict_test(self):\n",
    "        return self.X_test @ self.beta\n",
    "    \n",
    "    def MSE_train(self, prec=4):\n",
    "        return np.round(MSE(self.y_train,self.predict_train()),prec)\n",
    "        \n",
    "    def MSE_test(self, prec=4):\n",
    "        return np.round(MSE(self.y_test,self.predict_test()),prec)\n",
    "        \n",
    "    def R2_train(self, prec=4):\n",
    "        return np.round(R2(self.y_train,self.predict_train()),prec)\n",
    "        \n",
    "    def R2_test(self, prec=4):\n",
    "        return np.round(R2(self.y_test,self.predict_test()),prec)\n",
    "        \n",
    "    def Confidence_Interval(self, sigma=1):\n",
    "        #Calculates variance of beta, extracting just the diagonal elements of the matrix\n",
    "        #var(B_j)=sigma^2*(X^T*X)^{-1}_{jj}\n",
    "        beta_variance = np.diag(sigma**2 * np.linalg.pinv(self.X.T @ self.X))\n",
    "        ci1 = self.beta - 1.96 * np.sqrt(beta_variance)/(self.X.shape[0])\n",
    "        ci2 = self.beta + 1.96 * np.sqrt(beta_variance)/(self.X.shape[0])\n",
    "        print('Confidence interval of β-estimator at 95 %:')\n",
    "        ci_df = {r'$β_{-}$': ci1,\n",
    "                 r'$β_{ols}$': self.beta,\n",
    "                 r'$β_{+}$': ci2}\n",
    "        ci_df = pd.DataFrame(ci_df)\n",
    "        display(np.round(ci_df,3))\n",
    "        return ci1, ci2\n",
    "        \n",
    "\"\"\"\n",
    "    def predict(self, X):\n",
    "        Fit the model and return the prediction\n",
    "        \n",
    "        Args:\n",
    "        X (array): design matrix\n",
    "\n",
    "        Returns X*beta\n",
    "        \n",
    "        raise NotImplementedError(\"Method LinearRegression.predict is abstract and cannot be called\")\n",
    "\"\"\"\n",
    "\n",
    "class OLSRegression(LinearRegression):\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        super().__init__(X, y)\n",
    "        \n",
    "    def fit(self):\n",
    "        self.beta = np.linalg.pinv(self.X_train.T @ self.X_train) @ self.X_train.T @ self.y_train\n",
    "        return self.beta\n",
    "        \n",
    "    def fitSVD(self):\n",
    "        self.beta = SVDinv(self.X_train.T @ self.X_train) @ self.X_train.T @ self.y_train\n",
    "        return self.beta\n",
    "        \n",
    "    def fit_SK(self):\n",
    "        self.beta = SVDinv(self.X_train.T @ self.X_train) @ self.X_train.T @ self.y_train\n",
    "        return self.beta\n",
    "        \n",
    "    def gradient(X, y, beta):\n",
    "        return 2.0/X.shape[0] * X.T @ ((X @ beta) - y) # X.shape[0]=number of input (training) data\n",
    "            \n",
    "    def fitGD(self, eta = 0.1, Niterations = 1000):\n",
    "                \n",
    "        self.beta = GD(self.X_train, self.y_train, gradient=self.gradient, eta = eta, Niterations = Niterations)\n",
    "        \n",
    "        return self.beta\n",
    "    \n",
    "    def fitSGD(self, n_epochs, m, t0 = 5, t1 = 50):\n",
    "          \n",
    "        self.beta = SGD(X = self.X_train, y = self.y_train, gradient = self.gradient, n_epochs = n_epochs, m = m, t0 = t0, t1 = t1)\n",
    "        \n",
    "        return self.beta\n",
    "        \n",
    "    def fitSGD_SK(self):\n",
    "        \"\"\"\n",
    "        sgdreg = SGDRegressor(max_iter = 50, penalty=None, eta0=0.1)\n",
    "        sgdreg.fit(x,y.ravel())\n",
    "        self.beta = np.append(sgdreg.intercept_, sgdreg.coef_).reshape([self.X_train.shape[1],1])\n",
    "        \"\"\"\n",
    "        return self.beta\n",
    "        \n",
    "    def get_param(self):\n",
    "        return super().get_param()\n",
    "          \n",
    "    def predict_train(self):\n",
    "        return super().predict_train()\n",
    "        \n",
    "    def predict_test(self):\n",
    "        return super().predict_test()\n",
    "        \n",
    "    def MSE_train(self, prec=4):\n",
    "        return super().MSE_train(prec=prec)\n",
    "        \n",
    "    def MSE_test(self, prec=4):\n",
    "        return super().MSE_test(prec=prec)\n",
    "        \n",
    "    def R2_train(self, prec=4):\n",
    "        return super().R2_train(prec=prec)\n",
    "        \n",
    "    def R2_test(self, prec=4):\n",
    "        return super().R2_test(prec=prec)\n",
    "        \n",
    "    def Confidence_Interval(self, sigma=1):\n",
    "        return super().Confidence_Interval(sigma=sigma)\n",
    "        \n",
    "class RidgeRegression(LinearRegression):\n",
    "    \n",
    "    def __init__(self, X, y, lmb = 1e-12):\n",
    "        super().__init__(X, y)\n",
    "        self.lmd = lmd\n",
    "        \n",
    "    def set_lambda(self, lmb):\n",
    "        self.lmb=lmb\n",
    "        \n",
    "    def split(self, test_size=0.2):\n",
    "        return super().split(test_size=test_size)\n",
    "        \n",
    "    def rescale(self, with_std=False):\n",
    "        return super().rescale(with_std=with_std)\n",
    "    \n",
    "    def fit(self):\n",
    "        self.beta = np.linalg.pinv(self.X_train.T @ self.X_train + self.lmd * np.eye(len(self.X_train.T))) @ self.X_train.T @ self.y_train\n",
    "        return self.beta\n",
    "        \n",
    "    def fitSVD(self):\n",
    "        self.beta = SVDinv(self.X_train.T @ self.X_train + self.lmd * np.eye(len(self.X_train.T))) @ self.X_train.T @ self.y_train\n",
    "        return self.beta\n",
    "        \n",
    "    def fit_SK(self):\n",
    "        #self.beta = SVDinv(self.X_train.T @ self.X_train) @ self.X_train.T @ self.y_train\n",
    "        return self.beta\n",
    "        \n",
    "    def gradient(X, y, beta):\n",
    "        return #2.0/X.shape[0] * X.T @ ((X @ beta) - y) # X.shape[0]=number of input (training) data\n",
    "            \n",
    "    def fitGD(self, eta = 0.1, Niterations = 1000):\n",
    "                \n",
    "        self.beta = GD(self.X_train, self.y_train, gradient=self.gradient, eta = eta, Niterations = Niterations)\n",
    "        \n",
    "        return self.beta\n",
    "    \n",
    "    def fitSGD(self, n_epochs, m, t0 = 5, t1 = 50):\n",
    "          \n",
    "        self.beta = SGD(X = self.X_train, y = self.y_train, gradient = self.gradient, n_epochs = n_epochs, m = m, t0 = t0, t1 = t1)\n",
    "        \n",
    "        return self.beta\n",
    "        \n",
    "    def fitSGD_SK(self):\n",
    "        \"\"\"\n",
    "        sgdreg = SGDRegressor(max_iter = 50, penalty=None, eta0=0.1)\n",
    "        sgdreg.fit(x,y.ravel())\n",
    "        self.beta = np.append(sgdreg.intercept_, sgdreg.coef_).reshape([self.X_train.shape[1],1])\n",
    "        \"\"\"\n",
    "        return self.beta\n",
    "        \n",
    "    def get_param(self):\n",
    "        return super().get_param()\n",
    "          \n",
    "    def predict_train():\n",
    "        return super().predict_train()\n",
    "        \n",
    "    def predict_test(self):\n",
    "        return super().predict_test()\n",
    "\n",
    "    def MSE_train(self, prec=4):\n",
    "        return super().MSE_train(prec=prec)\n",
    "        \n",
    "    def MSE_test(self, prec=4):\n",
    "        return super().MSE_test(prec=prec)\n",
    "        \n",
    "    def R2_train(self, prec=4):\n",
    "        return super().R2_train(prec=prec)\n",
    "        \n",
    "    def R2_test(self, prec=4):\n",
    "        return super().R2_test(prec=prec)\n",
    "        \n",
    "    def Confidence_Interval(self, sigma=1):\n",
    "        return super().Confidence_Interval(sigma=sigma)\n",
    "\n",
    "class LassoRegression(LinearRegression):\n",
    "\n",
    "    def __init__(self, X, y, lmb = 1e-12):\n",
    "        super().__init__(X, y)\n",
    "        self.lmd = lmd\n",
    "        \n",
    "    def set_lambda(self, lmb):\n",
    "        self.lmb=lmb\n",
    "        \n",
    "    def split(self, test_size=0.2):\n",
    "        return super().split(test_size=test_size)\n",
    "        \n",
    "    def rescale(self, with_std=False):\n",
    "        return super().rescale(with_std=with_std)\n",
    "    \n",
    "    def fit(self):\n",
    "        RegLasso = linear_model.Lasso(self.lmd)\n",
    "        self.beta = RegLasso.fit(self.X_train,self.y_train)\n",
    "        return self.beta\n",
    "        \n",
    "    \"\"\"\n",
    "    def fit_SK(self):\n",
    "        return self.fit()\n",
    "        \n",
    "    def fitGD(self):\n",
    "        return self.fit()\n",
    "        \n",
    "    def fitSGD(self):\n",
    "        return self.fit()\n",
    "        \n",
    "    def gradient(X, y, beta):\n",
    "        return 2.0/X.shape[0] * X.T @ ((X @ beta) - y) # X.shape[0]=number of input (training) data\n",
    "            \n",
    "    def fitGD(self, eta = 0.1, Niterations = 1000):\n",
    "                \n",
    "        self.beta = GD(self.X_train, self.y_train, gradient=self.gradient, eta = eta, Niterations = Niterations)\n",
    "        \n",
    "        return self.beta\n",
    "    \n",
    "    def fitSGD(self, n_epochs, m, t0 = 5, t1 = 50):\n",
    "          \n",
    "        self.beta = SGD(X = self.X_train, y = self.y_train, gradient = self.gradient, n_epochs = n_epochs, m = m, t0 = t0, t1 = t1)\n",
    "        \n",
    "        return self.beta\n",
    "        \n",
    "    def fitSGD_SK(self):\n",
    "    \n",
    "        sgdreg = SGDRegressor(max_iter = 50, penalty=None, eta0=0.1)\n",
    "        sgdreg.fit(x,y.ravel())\n",
    "        self.beta = np.append(sgdreg.intercept_, sgdreg.coef_).reshape([self.X_train.shape[1],1])\n",
    "        \n",
    "        return self.beta\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_param(self):\n",
    "        return super().get_param()\n",
    "          \n",
    "    def predict_train(self):\n",
    "        return super().predict_train()\n",
    "        \n",
    "    def predict_test(self):\n",
    "        return super().predict_test()\n",
    "        \n",
    "    def MSE_train(self, prec=4):\n",
    "        return super().MSE_train(prec=prec)\n",
    "        \n",
    "    def MSE_test(self, prec=4):\n",
    "        return super().MSE_test(prec=prec)\n",
    "        \n",
    "    def R2_train(self, prec=4):\n",
    "        return super().R2_train(prec=prec)\n",
    "        \n",
    "    def R2_test(self, prec=4):\n",
    "        return super().R2_test(prec=prec)\n",
    "        \n",
    "    def Confidence_Interval(self, sigma=1):\n",
    "        return super().Confidence_Interval(sigma=sigma)\n",
    "    \n",
    "\"\"\"\n",
    "def ols_reg(X_train, X_test, y_train, y_test):\n",
    "\n",
    "\t# Calculating Beta Ordinary Least Square Equation with matrix pseudoinverse\n",
    "    # Altervatively to Numpy pseudoinverse it is possible to use the SVD theorem to evalute the inverse of a matrix (even in case it is singular). Just replace 'np.linalg.pinv' with 'SVDinv'.\n",
    "\tols_beta = np.linalg.pinv(X_train.T @ X_train) @ X_train.T @ y_train\n",
    "\n",
    "\ty_tilde = X_train @ ols_beta # y_prediction of the train data\n",
    "\ty_predict = X_test @ ols_beta # y_prediction of the test data\n",
    "  \n",
    "\treturn ols_beta, y_tilde, y_predict\n",
    " \n",
    " def ridge_reg(X_train, X_test, y_train, y_test, lmd = 10**(-12)):\n",
    " \n",
    "    ridge_beta = np.linalg.pinv(X_train.T @ X_train + lmd*np.eye(len(X_train.T))) @ X_train.T @ y_train #psudoinverse\n",
    "    y_model = X_train @ ridge_beta #calculates model\n",
    "    y_predict = X_test @ ridge_beta\n",
    "\n",
    "    #finds the lambda that gave the best MSE\n",
    "    #best_lamda = lambdas[np.where(MSE_values == np.min(MSE_values))[0]]\n",
    "\n",
    "    return ridge_beta, y_model, y_predict\n",
    "    \n",
    "def lasso_reg(X_train, X_test, y_train, y_test, lmd = 10**(-12)):\n",
    "\n",
    "    RegLasso = linear_model.Lasso(lmd)\n",
    "    _ = RegLasso.fit(X_train,y_train)\n",
    "    y_model = RegLasso.predict(X_train)\n",
    "    y_predict = RegLasso.predict(X_test)\n",
    "\n",
    "    return y_model, y_predict\n",
    "\"\"\"\n",
    "\n",
    "# Return the rolling mean of a vector and two values at one sigma from the rolling average\n",
    "def Rolling_Mean(vector, windows=3):\n",
    "    vector_df = pd.DataFrame({'vector': vector})\n",
    "    # computing the rolling average\n",
    "    rolling_mean = vector_df.vector.rolling(windows).mean().to_numpy()\n",
    "    # computing the values at two sigmas from the rolling average\n",
    "    rolling_std = vector_df.vector.rolling(windows).std().to_numpy()\n",
    "    value_up = rolling_mean + rolling_std\n",
    "    value_down = rolling_mean - rolling_std\n",
    "    \n",
    "    return rolling_mean, value_down, value_up\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "477ece61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_Xz(X_train, X_test, z_train, z_test, with_std=False):\n",
    "    scaler_X = StandardScaler(with_std=with_std) #with_std=False\n",
    "    scaler_X.fit(X_train)\n",
    "    X_train = scaler_X.transform(X_train)\n",
    "    X_test = scaler_X.transform(X_test)\n",
    "\n",
    "    scaler_z = StandardScaler(with_std=with_std) #with_std=False\n",
    "    z_train = np.squeeze(scaler_z.fit_transform(z_train.reshape(-1, 1))) #scaler_z.fit_transform(z_train) #\n",
    "    z_test = np.squeeze(scaler_z.transform(z_test.reshape(-1, 1))) #scaler_z.transform(z_test) #  \n",
    "    return X_train, X_test, z_train, z_test\n",
    "\n",
    "# Splitting and rescaling data (rescaling is optional)\n",
    "# Default values: 20% of test data and the scaler is StandardScaler without std.dev.\n",
    "def Split_and_Scale(X,z,test_size=0.2, scale=True, with_std=False):\n",
    "\n",
    "    #Splitting training and test data\n",
    "    X_train, X_test, z_train, z_test = train_test_split(X, z, test_size=test_size)\n",
    "\n",
    "    # Rescaling X and z (optional)\n",
    "    if scale:\n",
    "        X_train, X_test, z_train, z_test = scale_Xz(X_train, X_test, z_train, z_test, with_std=with_std)\n",
    "      \n",
    "    return X_train, X_test, z_train, z_test\n",
    "\n",
    "# OLS equation\n",
    "def OLS_solver(X_train, X_test, z_train, z_test):\n",
    "\n",
    "    # Calculating Beta Ordinary Least Square Equation with matrix pseudoinverse\n",
    "    # Altervatively to Numpy pseudoinverse it is possible to use the SVD theorem to evalute the inverse of a matrix (even in case it is singular). Just replace 'np.linalg.pinv' with 'SVDinv'.\n",
    "    ols_beta = np.linalg.pinv(X_train.T @ X_train) @ X_train.T @ z_train\n",
    "\n",
    "    z_tilde = X_train @ ols_beta # z_prediction of the train data\n",
    "    z_predict = X_test @ ols_beta # z_prediction of the test data\n",
    "  \n",
    "    return ols_beta, z_tilde, z_predict\n",
    "    \n",
    "def ridge_reg(X_train, X_test, z_train, z_test, lmd = 10**(-12)):\n",
    " \n",
    "    ridge_beta = np.linalg.pinv(X_train.T @ X_train + lmd*np.eye(len(X_train.T))) @ X_train.T @ z_train #psudoinverse\n",
    "    z_model = X_train @ ridge_beta #calculates model\n",
    "    z_predict = X_test @ ridge_beta\n",
    "\n",
    "    #finds the lambda that gave the best MSE\n",
    "    #best_lamda = lambdas[np.where(MSE_values == np.min(MSE_values))[0]]\n",
    "\n",
    "    return ridge_beta, z_model, z_predict\n",
    "    \n",
    "def lasso_reg(X_train, X_test, z_train, z_test, lmd = 10**(-12)):\n",
    "\n",
    "    RegLasso = linear_model.Lasso(lmd)\n",
    "    _ = RegLasso.fit(X_train,z_train)\n",
    "    z_model = RegLasso.predict(X_train)\n",
    "    z_predict = RegLasso.predict(X_test)\n",
    "\n",
    "    return z_model, z_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bb753532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.08033333  0.21241667 -0.004625    0.16577083  0.16637153\n",
      "  -0.06782841  0.07257494  0.12575231  0.09994083 -0.10346354  0.01118949\n",
      "   0.05754982  0.08036039  0.04393807 -0.12024953 -0.02415488  0.01295761\n",
      "   0.03514108  0.04366974  0.00213328]]\n",
      "[[ 0.         -0.08633333  0.17075    -0.17129167  0.03035417  0.10907986\n",
      "  -0.19398582 -0.05271441  0.0182581   0.04084071 -0.18911169 -0.08118609\n",
      "  -0.03601934 -0.0034974  -0.01027019 -0.17523442 -0.08607818 -0.05448448\n",
      "  -0.03435209 -0.02087356 -0.04449469]]\n",
      "[-0.30980687  0.03407068]\n",
      "[-0.32305563  0.00526445]\n",
      "[  0.           6.0240481    2.77833014 -27.41271571 -11.59605851\n",
      "  -4.54493467  38.33903483  34.4407874   17.5722142  -16.00905112]\n",
      "[-0.29419307 -0.0832303  -0.25361917  0.30578607 -0.08940484  0.35642934\n",
      " -0.19681158  0.5963452  -0.15675562  0.23819927]\n",
      "[-0.2410186   0.03289853 -0.32184282 -0.29944618  0.41458793  0.23171393\n",
      "  0.10950287 -0.07146335  0.67845615 -0.33666404]\n",
      "––––––––––––––––––––––––––––––––––––––––––––\n",
      "Train MSE: 0.0113\n",
      "Test MSE: 0.0111\n",
      "––––––––––––––––––––––––––––––––––––––––––––\n",
      "Train R2: 0.8808\n",
      "Test R2: 0.8678\n",
      "––––––––––––––––––––––––––––––––––––––––––––\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "# Degree of the polynomial\n",
    "degree=5\n",
    "# Datapoints (squared root of datapoints -> meshgrid)\n",
    "n = 25\n",
    "# Paramaters of noise distribution\n",
    "mu = 0; sigma = 0.1\n",
    "# Parameter of splitting data\n",
    "test_size = 0.2\n",
    "\n",
    "x,y,z=create_xyz_dataset(n,mu,sigma)\n",
    "#Plot_FrankeFunction(x,y,z)\n",
    "z=z.ravel()\n",
    "X=create_X(x,y,degree)\n",
    "\n",
    "X_train, X_test, z_train, z_test = Split_and_Scale(X,np.ravel(z)) #StardardScaler, test_size=0.2, scale=true\n",
    "print(X_train[:1])\n",
    "print(X_test[:1])\n",
    "print(z_train[:2])\n",
    "print(z_test[:2])\n",
    "\n",
    "ols_beta, z_tilde,z_predict = OLS_solver(X_train, X_test, z_train, z_test)\n",
    "\n",
    "print(ols_beta[:10])\n",
    "print(z_tilde[:10])\n",
    "print(z_predict[:10])\n",
    "\n",
    "prec=4\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "print(\"Train MSE:\", np.round(MSE(z_train,z_tilde),prec))\n",
    "print(\"Test MSE:\", np.round(MSE(z_test,z_predict),prec))\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "print(\"Train R2:\", np.round(R2(z_train,z_tilde),prec))\n",
    "print(\"Test R2:\", np.round(R2(z_test,z_predict),prec))\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e4e7fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.           6.0240481    2.77833014 -27.41271571 -11.59605851\n",
      "  -4.54493467  38.33903483  34.4407874   17.5722142  -16.00905112]\n",
      "[-0.29419307 -0.0832303  -0.25361917  0.30578607 -0.08940484  0.35642934\n",
      " -0.19681158  0.5963452  -0.15675562  0.23819927]\n",
      "[-0.2410186   0.03289853 -0.32184282 -0.29944618  0.41458793  0.23171393\n",
      "  0.10950287 -0.07146335  0.67845615 -0.33666404]\n",
      "––––––––––––––––––––––––––––––––––––––––––––\n",
      "Train MSE: 0.0113\n",
      "Test MSE: 0.0111\n",
      "––––––––––––––––––––––––––––––––––––––––––––\n",
      "Train R2: 0.8808\n",
      "Test R2: 0.8678\n",
      "––––––––––––––––––––––––––––––––––––––––––––\n",
      "Confidence interval of β-estimator at 95 %:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$β_{-}$</th>\n",
       "      <th>$β_{ols}$</th>\n",
       "      <th>$β_{+}$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.022</td>\n",
       "      <td>6.024</td>\n",
       "      <td>6.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.776</td>\n",
       "      <td>2.778</td>\n",
       "      <td>2.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-27.422</td>\n",
       "      <td>-27.413</td>\n",
       "      <td>-27.403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-11.603</td>\n",
       "      <td>-11.596</td>\n",
       "      <td>-11.589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-4.554</td>\n",
       "      <td>-4.545</td>\n",
       "      <td>-4.535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38.317</td>\n",
       "      <td>38.339</td>\n",
       "      <td>38.361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>34.425</td>\n",
       "      <td>34.441</td>\n",
       "      <td>34.457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17.556</td>\n",
       "      <td>17.572</td>\n",
       "      <td>17.588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-16.031</td>\n",
       "      <td>-16.009</td>\n",
       "      <td>-15.987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-19.008</td>\n",
       "      <td>-18.985</td>\n",
       "      <td>-18.963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-38.885</td>\n",
       "      <td>-38.868</td>\n",
       "      <td>-38.851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-7.497</td>\n",
       "      <td>-7.481</td>\n",
       "      <td>-7.466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-25.695</td>\n",
       "      <td>-25.678</td>\n",
       "      <td>-25.661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>35.447</td>\n",
       "      <td>35.469</td>\n",
       "      <td>35.492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.588</td>\n",
       "      <td>1.597</td>\n",
       "      <td>1.605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>11.903</td>\n",
       "      <td>11.911</td>\n",
       "      <td>11.918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9.857</td>\n",
       "      <td>9.865</td>\n",
       "      <td>9.872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-3.961</td>\n",
       "      <td>-3.953</td>\n",
       "      <td>-3.946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13.958</td>\n",
       "      <td>13.965</td>\n",
       "      <td>13.973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-17.986</td>\n",
       "      <td>-17.977</td>\n",
       "      <td>-17.968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    $β_{-}$  $β_{ols}$  $β_{+}$\n",
       "0    -0.000      0.000    0.000\n",
       "1     6.022      6.024    6.026\n",
       "2     2.776      2.778    2.780\n",
       "3   -27.422    -27.413  -27.403\n",
       "4   -11.603    -11.596  -11.589\n",
       "5    -4.554     -4.545   -4.535\n",
       "6    38.317     38.339   38.361\n",
       "7    34.425     34.441   34.457\n",
       "8    17.556     17.572   17.588\n",
       "9   -16.031    -16.009  -15.987\n",
       "10  -19.008    -18.985  -18.963\n",
       "11  -38.885    -38.868  -38.851\n",
       "12   -7.497     -7.481   -7.466\n",
       "13  -25.695    -25.678  -25.661\n",
       "14   35.447     35.469   35.492\n",
       "15    1.588      1.597    1.605\n",
       "16   11.903     11.911   11.918\n",
       "17    9.857      9.865    9.872\n",
       "18   -3.961     -3.953   -3.946\n",
       "19   13.958     13.965   13.973\n",
       "20  -17.986    -17.977  -17.968"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "––––––––––––––––––––––––––––––––––––––––––––\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "# Degree of the polynomial\n",
    "degree=5\n",
    "# Datapoints (squared root of datapoints -> meshgrid)\n",
    "n = 25\n",
    "# Paramaters of noise distribution\n",
    "mu = 0; sigma = 0.1\n",
    "# Parameter of splitting data\n",
    "test_size = 0.2\n",
    "\n",
    "x,y,z=create_xyz_dataset(n,mu,sigma)\n",
    "#Plot_FrankeFunction(x,y,z)\n",
    "z=z.ravel()\n",
    "X=create_X(x,y,degree)\n",
    "\n",
    "model=OLSRegression(X,z)\n",
    "model.split().rescale()\n",
    "\n",
    "beta = model.fit()\n",
    "print(beta[:10])\n",
    "z_tilde = model.predict_train()\n",
    "z_predict = model.predict_test()\n",
    "print(z_tilde[:10])\n",
    "print(z_predict[:10])\n",
    "\n",
    "\"\"\"\n",
    "prec=4\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "print(\"Train MSE:\", np.round(MSE(z_train,z_tilde),prec))\n",
    "print(\"Test MSE:\", np.round(MSE(z_test,z_predict),prec))\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "print(\"Train R2:\", np.round(R2(z_train,z_tilde),prec))\n",
    "print(\"Test R2:\", np.round(R2(z_test,z_predict),prec))\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "\"\"\"\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "print(\"Train MSE:\", model.MSE_train())\n",
    "print(\"Test MSE:\", model.MSE_test())\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "print(\"Train R2:\", model.R2_train())\n",
    "print(\"Test R2:\", model.R2_test())\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "# Confidence interval\n",
    "beta1, beta2 = model.Confidence_Interval(0.1)\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7467da2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Own inversion\n",
      "[3.92838935 3.01524364]\n",
      "[3.92838935 3.01524364]\n",
      "sgdreg from scikit\n",
      "[3.98089452 3.03544864]\n",
      "theta from own gd\n",
      "[[3.94443518]\n",
      " [2.99177916]]\n",
      "theta from own sdg\n",
      "[[3.91649399]\n",
      " [2.98322362]]\n",
      "3.928389345935617\n",
      "2.035140218790668\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Importing various packages\n",
    "from math import exp, sqrt\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "n = 1000\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "\n",
    "\n",
    "model=OLSRegression(X,y.ravel())\n",
    "intercept = model.split().fit()[0]\n",
    "std = np.std(model.y_train)\n",
    "\n",
    "#model.rescale()\n",
    "\n",
    "theta_linreg = np.linalg.pinv(model.X_train.T @ model.X_train) @ (model.X_train.T @ model.y_train)\n",
    "beta_linreg = model.fit()\n",
    "\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "print(beta_linreg)\n",
    "\n",
    "\n",
    "sgdreg = SGDRegressor(max_iter = 50, penalty=None, eta0=0.1,fit_intercept=False)\n",
    "sgdreg.fit(X, y.ravel())\n",
    "print(\"sgdreg from scikit\")\n",
    "\n",
    "beta = sgdreg.coef_\n",
    "if False:\n",
    "    beta[0] = sgdreg.intercept_\n",
    "            \n",
    "print(beta)\n",
    "\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "eta = 0.1\n",
    "Niterations = 1000\n",
    "\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = 2.0/n * X.T @ ((X @ theta)-y)\n",
    "    theta -= eta*gradients\n",
    "print(\"theta from own gd\")\n",
    "print(theta)\n",
    "\n",
    "xnew = np.array([[0],[2]])\n",
    "Xnew = np.c_[np.ones((2,1)), xnew]\n",
    "ypredict = X.dot(theta)\n",
    "ypredict2 = X.dot(theta_linreg)\n",
    "plt.plot(x, ypredict, \"r-\")\n",
    "plt.plot(x, ypredict2, \"b-\")\n",
    "\n",
    "\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "t0, t1 = 5, 50\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2.0 * xi.T @ ((xi @ theta)-yi)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        theta = theta - eta*gradients\n",
    "print(\"theta from own sdg\")\n",
    "print(theta)\n",
    "\n",
    "plt.plot(x, y ,'go') \n",
    "\n",
    "std_x=np.std(x)\n",
    "m_x=np.mean(x)\n",
    "scaler_X = StandardScaler()\n",
    "scaler_X.fit(x)\n",
    "x = scaler_X.transform(x)\n",
    "\n",
    "m_y=np.mean(y)\n",
    "std_y=np.std(y)\n",
    "scaler_y = StandardScaler()\n",
    "y2 = scaler_y.fit_transform(y.reshape(-1, 1)) #scaler_y.fit_transform(y_train) #\"\"\"\n",
    "\n",
    "print(intercept)\n",
    "print(std)\n",
    "plt.plot(m_x+x*std_x, m_y+y2.ravel()*std_y ,'ro')\n",
    "#plt.axis([0,2.0,0, 15.0])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.title(r'Random numbers ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fcb114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
