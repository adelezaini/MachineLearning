{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://github.com/UdiBhaskar/Deep-Learning/blob/master/DNN%20in%20python%20from%20scratch.ipynb\n",
    "and https://static.latexstudio.net/article/2018/0912/neuralnetworksanddeeplearning.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Developing a code for doing neural networks with back propagation\n",
    "\n",
    "\n",
    "One can identify a set of key steps when using neural networks to solve supervised learning problems:  \n",
    "\n",
    "1. Collect and pre-process data  \n",
    "\n",
    "2. Define model and architecture  \n",
    "\n",
    "3. Choose cost function and optimizer  \n",
    "\n",
    "4. Train the model  \n",
    "\n",
    "5. Evaluate model performance on test data  \n",
    "\n",
    "6. Adjust hyperparameters (if necessary, network architecture)\n",
    "\n",
    "## Collect and pre-process data\n",
    "\n",
    "$X = (n_{inputs}, n_{features})$\n",
    "$Y = (n_{inputs}, n_{categories})$\n",
    "\n",
    "```\n",
    "# flatten the image\n",
    "# the value -1 means dimension is inferred from the remaining dimensions: 8x8 = 64\n",
    "n_inputs = len(inputs)\n",
    "inputs = inputs.reshape(n_inputs, -1)\n",
    "```\n",
    "\n",
    "## Test and train\n",
    "```\n",
    "train_size = 0.8\n",
    "test_size = 1 - train_size\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(inputs, labels, train_size=train_size,test_size=test_size)\n",
    "```\n",
    "\n",
    "## etc... week41\n",
    "                                                    \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-1.09129556],\n",
       "        [ 0.88893389]]),\n",
       " 'b1': array([[ 1.21914541],\n",
       "        [-0.47451073]]),\n",
       " 'W2': array([[ 1.11081672, -0.83216592],\n",
       "        [ 0.13413967, -1.77965204]]),\n",
       " 'b2': array([[-0.36493927],\n",
       "        [ 0.14131625]])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "layer_dims = [1,2,2]\n",
    "parameters = {}\n",
    "L = len(layer_dims)\n",
    "for l in range(1, L):\n",
    "    parameters['W' + str(l)] = np.random.normal(0,np.sqrt(2.0/layer_dims[l-1]),(layer_dims[l], layer_dims[l-1]))\n",
    "    parameters['b' + str(l)] = np.random.normal(0,np.sqrt(2.0/layer_dims[l-1]),(layer_dims[l], 1))\n",
    "\n",
    "display(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm\n",
    "\n",
    "General steps to build neural network:\n",
    "Define the neural network structure ( # of input units, # of hidden units, etc)\n",
    "Initialize the model's parameters\n",
    "Loop:\n",
    "Implement forward propagation\n",
    "Compute loss\n",
    "Implement backward propagation to get the gradients\n",
    "Update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(layer_dims,init_type='he_normal',seed=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "            layer_dims lis is like  [ no of input features,# of neurons in hidden layer-1,..,\n",
    "                                     # of neurons in hidden layer-n shape,output]\n",
    "            init_type -- he_normal  --> N(0,sqrt(2/fanin))\n",
    "                         he_uniform --> Uniform(-sqrt(6/fanin),sqrt(6/fanin))\n",
    "                         xavier_normal --> N(0,2/(fanin+fanout))\n",
    "                         xavier_uniform --> Uniform(-sqrt(6/fanin+fanout),sqrt(6/fanin+fanout))\n",
    "                         seed -- random seed to generate weights\n",
    "        Returns:\n",
    "            parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        parameters = {}\n",
    "        opt_parameters = {}\n",
    "        L = len(layer_dims)            # number of layers in the network\n",
    "        if  init_type == 'he_normal':\n",
    "            for l in range(1, L):\n",
    "                parameters['W' + str(l)] = np.random.normal(0,np.sqrt(2.0/layer_dims[l-1]),(layer_dims[l], layer_dims[l-1]))\n",
    "                parameters['b' + str(l)] = np.random.normal(0,np.sqrt(2.0/layer_dims[l-1]),(layer_dims[l], 1))  \n",
    "            \n",
    "        elif init_type == 'he_uniform':\n",
    "            for l in range(1, L):\n",
    "                parameters['W' + str(l)] = np.random.uniform(-np.sqrt(6.0/layer_dims[l-1]),\n",
    "                                                        np.sqrt(6.0/layer_dims[l-1]),\n",
    "                                                        (layer_dims[l], layer_dims[l-1]))\n",
    "                parameters['b' + str(l)] = np.random.uniform(-np.sqrt(6.0/layer_dims[l-1]),\n",
    "                                                        np.sqrt(6.0/layer_dims[l-1]),\n",
    "                                                        (layer_dims[l], 1))\n",
    "            \n",
    "        elif init_type == 'xavier_normal':\n",
    "            for l in range(1, L):\n",
    "                parameters['W' + str(l)] = np.random.normal(0,2.0/(layer_dims[l]+layer_dims[l-1]),\n",
    "                                                                   (layer_dims[l], layer_dims[l-1]))\n",
    "                parameters['b' + str(l)] = np.random.normal(0,2.0/(layer_dims[l]+layer_dims[l-1]),\n",
    "                                                                      (layer_dims[l], 1)) \n",
    "            \n",
    "        elif init_type == 'xavier_uniform':\n",
    "            for l in range(1, L):\n",
    "                parameters['W' + str(l)] = np.random.uniform(-(np.sqrt(6.0/(layer_dims[l]+layer_dims[l-1]))),\n",
    "                                                        (np.sqrt(6.0/(layer_dims[l]+layer_dims[l-1]))),\n",
    "                                                        (layer_dims[l], layer_dims[l-1]))\n",
    "                parameters['b' + str(l)] = np.random.uniform(-(np.sqrt(6.0/(layer_dims[l]+layer_dims[l-1]))),\n",
    "                                                        (np.sqrt(6.0/(layer_dims[l]+layer_dims[l-1]))),\n",
    "                                                        (layer_dims[l], 1))\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "def forward_propagation(X, hidden_layers,parameters,keep_proba=1,seed=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    hidden_layers -- List of hideden layers\n",
    "    weights -- Output of weights_init dict (parameters)\n",
    "    keep_prob -- probability of keeping a neuron active during drop-out, scalar\n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "    if seed != None:\n",
    "        np.random.seed(seed)\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(hidden_layers)\n",
    "    for l,active_function in enumerate(hidden_layers,start=1):\n",
    "        A_prev = A \n",
    "        \n",
    "        Z = np.dot(parameters['W' + str(l)],A_prev)+parameters['b' + str(l)]\n",
    "        \n",
    "        if active_function == \"sigmoid\":\n",
    "            A = sigmoid(Z)\n",
    "        elif active_function == \"relu\":\n",
    "            A = ReLU(Z)\n",
    "        elif active_function == \"tanh\":\n",
    "            A = Tanh(Z)\n",
    "        elif active_function == \"softmax\":\n",
    "            A = softmax(Z)\n",
    "            \n",
    "        if keep_proba != 1 and l != L and l != 1:\n",
    "            D = np.random.rand(A.shape[0],A.shape[1])\n",
    "            D = (D<keep_prob)\n",
    "            A = np.multiply(A,D)\n",
    "            A = A / keep_prob\n",
    "            cache = ((A_prev, parameters['W' + str(l)],parameters['b' + str(l)],D), Z)\n",
    "            caches.append(cache_temp)\n",
    "        else:\n",
    "            cache = ((A_prev, parameters['W' + str(l)],parameters['b' + str(l)]), Z)\n",
    "            #print(A.shape)\n",
    "            caches.append(cache)\n",
    "            \n",
    "    return A, caches\n",
    "\n",
    "def compute_cost(A, Y, parameters, lamda=0,penality=None):\n",
    "    \"\"\"\n",
    "    Implement the cost function with L2 regularization. See formula (2) above.\n",
    "    \n",
    "    Arguments:\n",
    "    A -- post-activation, output of forward propagation\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    parameters -- python dictionary containing parameters of the model\n",
    "    \n",
    "    Returns:\n",
    "    cost - value of the regularized loss function \n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = np.squeeze(-np.sum(np.multiply(np.log(A),Y))/m)\n",
    "    \n",
    "    L = len(parameters)//2\n",
    "    \n",
    "    if penality == 'l2' and lamda != 0:\n",
    "        sum_weights = 0\n",
    "        for l in range(1, L):\n",
    "            sum_weights = sum_weights + np.sum(np.square(parameters['W' + str(l)]))\n",
    "        cost = cost + sum_weights * (lambd/(2*m))\n",
    "    elif penality == 'l1' and lamda != 0:\n",
    "        sum_weights = 0\n",
    "        for l in range(1, L):\n",
    "            sum_weights = sum_weights + np.sum(np.abs(parameters['W' + str(l)]))\n",
    "        cost = cost + sum_weights * (lambd/(2*m))\n",
    "    \n",
    "    return cost\n",
    "def back_propagation(AL, Y, caches, hidden_layers, keep_prob=1, penality=None,lamda=0):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "    hidden_layers -- hidden layer names\n",
    "    keep_prob -- probabaility for dropout\n",
    "    penality -- regularization penality 'l1' or 'l2' or None\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    \n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dZL = AL - Y\n",
    "    \n",
    "    cache = caches[L-1]\n",
    "    linear_cache, activation_cache = cache\n",
    "    AL, W, b = linear_cache\n",
    "    grads[\"dW\" + str(L)] = np.dot(dZL,AL.T)/m\n",
    "    grads[\"db\" + str(L)] = np.sum(dZL,axis=1,keepdims=True)/m\n",
    "    grads[\"dA\" + str(L-1)] = np.dot(W.T,dZL)\n",
    "    \n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    v_dropout = 0\n",
    "    for l in reversed(range(L-1)):\n",
    "        cache = caches[l]\n",
    "        active_function = hidden_layers[l]\n",
    "        \n",
    "        linear_cache, Z = cache\n",
    "        try:\n",
    "            A_prev, W, b = linear_cache\n",
    "        except:\n",
    "            A_prev, W, b, D = linear_cache\n",
    "            v_dropout = 1\n",
    "            \n",
    "        m = A_prev.shape[1]\n",
    "        \n",
    "        if keep_prob != 1 and v_dropout == 1:\n",
    "            dA_prev = np.multiply(grads[\"dA\" + str(l + 1)],D)\n",
    "            dA_prev = dA_prev/keep_prob\n",
    "            v_dropout = 0\n",
    "        else:\n",
    "            dA_prev = grads[\"dA\" + str(l + 1)]\n",
    "            v_dropout = 0\n",
    "            \n",
    "        if active_function == \"sigmoid\":\n",
    "            dZ = np.multiply(dA_prev,sigmoid(Z,derivative=True))\n",
    "        elif active_function == \"relu\":\n",
    "            dZ = np.multiply(dA_prev,ReLU(Z,derivative=True))\n",
    "        elif active_function == \"tanh\":\n",
    "            dZ = np.multiply(dA_prev,Tanh(Z,derivative=True))\n",
    "            \n",
    "            \n",
    "        grads[\"dA\" + str(l)] = np.dot(W.T,dZ)\n",
    "        \n",
    "        if penality == 'l2':\n",
    "            grads[\"dW\" + str(l + 1)] = (np.dot(dZ,A_prev.T)/m)  + ((lambd * W)/m)\n",
    "        elif penality == 'l1':\n",
    "            grads[\"dW\" + str(l + 1)] = (np.dot(dZ,A_prev.T)/m)  + ((lambd * np.sign(W+10**-8))/m)\n",
    "        else:\n",
    "            grads[\"dW\" + str(l + 1)] = (np.dot(dZ,A_prev.T)/m)\n",
    "            \n",
    "        grads[\"db\" + str(l + 1)] = np.sum(dZ,axis=1,keepdims=True)/m\n",
    "        \n",
    "        \n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads,learning_rate,iter_no,method = 'SGD',opt_params=None,beta1=0.9,beta2=0.999):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    method -- method for updation of weights\n",
    "              'SGD','SGDM','RMSP','ADAM'\n",
    "    learning rate -- learning rate alpha value\n",
    "    beta1 -- weighted avg parameter for SGDM and ADAM\n",
    "    beta2 -- weighted avg parameter for RMSP and ADAM\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    if method == 'SGD':\n",
    "        for l in range(L):\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l + 1)]\n",
    "            parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l + 1)]\n",
    "    elif method == 'SGDM':\n",
    "        for l in range(L):\n",
    "            opt_parameters['vdb'+str(l+1)] = beta1*opt_parameters['vdb'+str(l+1)] + (1-beta1)*grads[\"db\" + str(l + 1)]\n",
    "            opt_parameters['vdw'+str(l+1)] = beta1*opt_parameters['vdw'+str(l+1)] + (1-beta1)*grads[\"dW\" + str(l + 1)]\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*opt_parameters['vdw'+str(l+1)]\n",
    "            parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*opt_parameters['vdb'+str(l+1)]\n",
    "    elif method == 'RMSP':\n",
    "        for l in range(L):\n",
    "            opt_parameters['sdb'+str(l+1)] = beta2*opt_parameters['sdb'+str(l+1)] + (1-beta2)*np.square(grads[\"db\" + str(l + 1)])\n",
    "            opt_parameters['sdw'+str(l+1)] = beta2*opt_parameters['sdw'+str(l+1)] + (1-beta2)*np.square(grads[\"dW\" + str(l + 1)])\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - \\\n",
    "                                       learning_rate*(grads[\"dW\" + str(l + 1)]/(np.sqrt(opt_parameters['sdw'+str(l+1)])+10**-8))\n",
    "            parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - \\\n",
    "                                       learning_rate*(grads[\"db\" + str(l + 1)]/(np.sqrt(opt_parameters['sdb'+str(l+1)])+10**-8))\n",
    "    elif method == 'ADAM':\n",
    "        for l in range(L):\n",
    "            opt_parameters['vdb'+str(l+1)] = beta1*opt_parameters['vdb'+str(l+1)] + (1-beta1)*grads[\"db\" + str(l + 1)]\n",
    "            opt_parameters['vdw'+str(l+1)] = beta1*opt_parameters['vdw'+str(l+1)] + (1-beta1)*grads[\"dW\" + str(l + 1)]\n",
    "            opt_parameters['sdb'+str(l+1)] = beta2*opt_parameters['sdb'+str(l+1)] + (1-beta2)*np.square(grads[\"db\" + str(l + 1)])\n",
    "            opt_parameters['sdw'+str(l+1)] = beta2*opt_parameters['sdw'+str(l+1)] + (1-beta2)*np.square(grads[\"dW\" + str(l + 1)])\n",
    "            \n",
    "            learningrate = learning_rate * np.sqrt((1-beta2**iter_no)/((1-beta1**iter_no)+10**-8))\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - \\\n",
    "                                       learning_rate*(opt_parameters['vdw'+str(l+1)]/\\\n",
    "                                                      (np.sqrt(opt_parameters['sdw'+str(l+1)])+10**-8))\n",
    "            parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - \\\n",
    "                                       learning_rate*(opt_parameters['vdb'+str(l+1)]/\\\n",
    "                                                      (np.sqrt(opt_parameters['sdb'+str(l+1)])+10**-8))\n",
    "        \n",
    "    return parameters,opt_parameters\n",
    "\n",
    "def predict(parameters, X,hidden_layers,return_prob=False):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    A, cache = forward_propagation(X,hidden_layers,parameters,seed=3)\n",
    "    if return_prob == True:\n",
    "        return A\n",
    "    else:\n",
    "        return np.argmax(A, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All in one object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code was inspired by the source:  https://github.com/UdiBhaskar/Deep-Learning/blob/master/DNN%20in%20python%20from%20scratch.ipynb . I really appreciated the completeness and the tidiness of it.\n",
    "\n",
    "The idea behind is to create an object 'NeuralNetwork' to perform ... When initialiazing it, it is basically building the architecture of the DNN (n of layers, n of neurons for each layer, activation functions), while when fitting it (fit()), X and y are provided so to create the specific case to perform the analysis.\n",
    "The methods of the class are:\n",
    "-\n",
    "-\n",
    "-\n",
    "For any further details on the class and its usage look at the documentation of the code [link to the class].\n",
    "\n",
    "For further info on the optimizers: https://ruder.io/optimizing-gradient-descent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNClassifier(object):\n",
    "    '''\n",
    "    Parameters: layer_dims -- List Dimensions of layers including input and output layer\n",
    "                hidden_layers -- List of hidden layers\n",
    "                                 'relu','sigmoid','tanh','softplus','arctan','elu','identity','softmax'\n",
    "                                 Note: 1. last layer must be softmax \n",
    "                                       2. For relu and elu need to mention alpha value as below\n",
    "                                        ['tanh',('relu',alpha1),('elu',alpha2),('relu',alpha3),'softmax']\n",
    "                                        need to give a tuple for relu and elu if you want to mention alpha\n",
    "                                        if not default alpha is 0\n",
    "                init_type -- init_type -- he_normal  --> N(0,sqrt(2/fanin))\n",
    "                             he_uniform --> Uniform(-sqrt(6/fanin),sqrt(6/fanin))\n",
    "                             xavier_normal --> N(0,2/(fanin+fanout))\n",
    "                             xavier_uniform --> Uniform(-sqrt(6/fanin+fanout),sqrt(6/fanin+fanout))\n",
    "                                 \n",
    "                learning_rate -- Learning rate\n",
    "                optimization_method -- optimization method 'SGD','SGDM','RMSP','ADAM'\n",
    "                batch_size -- Batch size to update weights \n",
    "                max_epoch -- Max epoch number\n",
    "                             Note : Max_iter  = max_epoch * (size of traing / batch size)\n",
    "                tolarance -- if abs(previous cost  - current cost ) < tol training will be stopped\n",
    "                             if None -- No check will be performed\n",
    "                keep_proba -- probability for dropout\n",
    "                              if 1 then there is no dropout\n",
    "                penality -- regularization penality\n",
    "                            values taken 'l1','l2',None(default)\n",
    "                lamda -- l1 or l2 regularization value\n",
    "                beta1 -- SGDM and adam optimization param\n",
    "                beta2 -- RMSP and adam optimization value\n",
    "                seed -- Random seed to generate randomness\n",
    "                verbose -- takes 0  or 1 \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,layer_dims,hidden_layers,init_type='he_normal',learning_rate=0.1,\n",
    "                 optimization_method = 'SGD',batch_size=64,max_epoch=100,tolarance = 0.00001,\n",
    "                 keep_proba=1,penality=None,lamda=0,beta1=0.9,\n",
    "                 beta2=0.999,seed=None,verbose=0):\n",
    "        self.layer_dims = layer_dims\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.init_type = init_type\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimization_method = optimization_method\n",
    "        self.batch_size = batch_size\n",
    "        self.keep_proba = keep_proba\n",
    "        self.penality = penality\n",
    "        self.lamda = lamda\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.seed = seed\n",
    "        self.max_epoch = max_epoch\n",
    "        self.tol = tolarance\n",
    "        self.verbose = verbose\n",
    "    @staticmethod\n",
    "    def weights_init(layer_dims,init_type='he_normal',seed=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "            layer_dims lis is like  [ no of input features,# of neurons in hidden layer-1,..,\n",
    "                                     # of neurons in hidden layer-n shape,output]\n",
    "            init_type -- he_normal  --> N(0,sqrt(2/fanin))\n",
    "                         he_uniform --> Uniform(-sqrt(6/fanin),sqrt(6/fanin))\n",
    "                         xavier_normal --> N(0,2/(fanin+fanout))\n",
    "                         xavier_uniform --> Uniform(-sqrt(6/fanin+fanout),sqrt(6/fanin+fanout))\n",
    "                         seed -- random seed to generate weights\n",
    "        Returns:\n",
    "            parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        parameters = {}\n",
    "        opt_parameters = {}\n",
    "        L = len(layer_dims)            # number of layers in the network\n",
    "        if  init_type == 'he_normal':\n",
    "            for l in range(1, L):\n",
    "                parameters['W' + str(l)] = np.random.normal(0,np.sqrt(2.0/layer_dims[l-1]),(layer_dims[l], layer_dims[l-1]))\n",
    "                parameters['b' + str(l)] = np.random.normal(0,np.sqrt(2.0/layer_dims[l-1]),(layer_dims[l], 1))  \n",
    "            \n",
    "        elif init_type == 'he_uniform':\n",
    "            for l in range(1, L):\n",
    "                parameters['W' + str(l)] = np.random.uniform(-np.sqrt(6.0/layer_dims[l-1]),\n",
    "                                                        np.sqrt(6.0/layer_dims[l-1]),\n",
    "                                                        (layer_dims[l], layer_dims[l-1]))\n",
    "                parameters['b' + str(l)] = np.random.uniform(-np.sqrt(6.0/layer_dims[l-1]),\n",
    "                                                        np.sqrt(6.0/layer_dims[l-1]),\n",
    "                                                        (layer_dims[l], 1))\n",
    "            \n",
    "        elif init_type == 'xavier_normal':\n",
    "            for l in range(1, L):\n",
    "                parameters['W' + str(l)] = np.random.normal(0,2.0/(layer_dims[l]+layer_dims[l-1]),\n",
    "                                                                   (layer_dims[l], layer_dims[l-1]))\n",
    "                parameters['b' + str(l)] = np.random.normal(0,2.0/(layer_dims[l]+layer_dims[l-1]),\n",
    "                                                                      (layer_dims[l], 1)) \n",
    "            \n",
    "        elif init_type == 'xavier_uniform':\n",
    "            for l in range(1, L):\n",
    "                parameters['W' + str(l)] = np.random.uniform(-(np.sqrt(6.0/(layer_dims[l]+layer_dims[l-1]))),\n",
    "                                                        (np.sqrt(6.0/(layer_dims[l]+layer_dims[l-1]))),\n",
    "                                                        (layer_dims[l], layer_dims[l-1]))\n",
    "                parameters['b' + str(l)] = np.random.uniform(-(np.sqrt(6.0/(layer_dims[l]+layer_dims[l-1]))),\n",
    "                                                        (np.sqrt(6.0/(layer_dims[l]+layer_dims[l-1]))),\n",
    "                                                        (layer_dims[l], 1))\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(X,derivative=False):\n",
    "        '''Compute Sigmaoid and its derivative'''\n",
    "        if derivative == False:\n",
    "            out = 1 / (1 + np.exp(-np.array(X)))\n",
    "        elif derivative == True:\n",
    "            s = 1 / (1 + np.exp(-np.array(X)))\n",
    "            out = s*(1-s)\n",
    "        return out\n",
    "    @staticmethod\n",
    "    def ReLU(X,alpha=0,derivative=False):\n",
    "        '''Compute ReLU function and derivative'''\n",
    "        X = np.array(X,dtype=np.float64)\n",
    "        if derivative == False:\n",
    "            return np.where(X<0,alpha*X,X)\n",
    "        elif derivative == True:\n",
    "            X_relu = np.ones_like(X,dtype=np.float64)\n",
    "            X_relu[X < 0] = alpha\n",
    "            return X_relu\n",
    "    @staticmethod\n",
    "    def Tanh(X,derivative=False):\n",
    "        '''Compute tanh values and derivative of tanh'''\n",
    "        X = np.array(X)\n",
    "        if derivative == False:\n",
    "            return np.tanh(X)\n",
    "        if derivative == True:\n",
    "            return 1 - (np.tanh(X))**2\n",
    "    @staticmethod\n",
    "    def softplus(X,derivative=False):\n",
    "        '''Compute tanh values and derivative of tanh'''\n",
    "        X = np.array(X)\n",
    "        if derivative == False:\n",
    "            return np.log(1+np.exp(X))\n",
    "        if derivative == True:\n",
    "            return 1 / (1 + np.exp(-np.array(X)))\n",
    "    @staticmethod\n",
    "    def arctan(X,derivative=False):\n",
    "        '''Compute tan^-1(X) and derivative'''\n",
    "        if derivative == False:\n",
    "            return  np.arctan(X)\n",
    "        if derivative == True:\n",
    "            return 1/ (1 + np.square(X))\n",
    "    @staticmethod\n",
    "    def identity(X,derivative=False):\n",
    "        '''identity function and derivative f(x) = x'''\n",
    "        X = np.array(X)\n",
    "        if derivative ==  False:\n",
    "            return X\n",
    "        if derivative == True:\n",
    "            return np.ones_like(X)\n",
    "    @staticmethod\n",
    "    def elu(X,alpha=0,derivative=False):\n",
    "        '''Exponential Linear Unit'''\n",
    "        X = np.array(X,dtype=np.float64)\n",
    "        if derivative == False:\n",
    "            return np.where(X<0,alpha*(np.exp(X)-1),X)\n",
    "        elif derivative == True:\n",
    "            return np.where(X<0,alpha*(np.exp(X)),1)\n",
    "    @staticmethod\n",
    "    def softmax(X):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        return np.exp(X) / np.sum(np.exp(X),axis=0)\n",
    "    @staticmethod\n",
    "    def forward_propagation(X, hidden_layers,parameters,keep_prob=1,seed=None):\n",
    "    \n",
    "        \"\"\"\"\n",
    "        Arguments:\n",
    "            X -- data, numpy array of shape (input size, number of examples)\n",
    "            hidden_layers -- List of hideden layers\n",
    "            weights -- Output of weights_init dict (parameters)\n",
    "            keep_prob -- probability of keeping a neuron active during drop-out, scalar\n",
    "        Returns:\n",
    "            AL -- last post-activation value\n",
    "            caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "        \"\"\"\n",
    "        if seed != None:\n",
    "            np.random.seed(seed)\n",
    "        caches = []\n",
    "        A = X\n",
    "        L = len(hidden_layers)\n",
    "        for l,active_function in enumerate(hidden_layers,start=1):\n",
    "            A_prev = A \n",
    "        \n",
    "            Z = np.dot(parameters['W' + str(l)],A_prev)+parameters['b' + str(l)]\n",
    "            \n",
    "            if type(active_function) is tuple:\n",
    "                \n",
    "                if  active_function[0] == \"relu\":\n",
    "                    A = DNNClassifier.ReLU(Z,active_function[1])\n",
    "                elif active_function[0] == 'elu':\n",
    "                    A = DNNClassifier.elu(Z,active_function[1])\n",
    "            else:\n",
    "                if active_function == \"sigmoid\":\n",
    "                    A = DNNClassifier.sigmoid(Z)\n",
    "                elif active_function == \"identity\":\n",
    "                    A = DNNClassifier.identity(Z)\n",
    "                elif active_function == \"arctan\":\n",
    "                    A = DNNClassifier.arctan(Z)\n",
    "                elif active_function == \"softplus\":\n",
    "                    A = DNNClassifier.softplus(Z)\n",
    "                elif active_function == \"tanh\":\n",
    "                    A = DNNClassifier.Tanh(Z)\n",
    "                elif active_function == \"softmax\":\n",
    "                    A = DNNClassifier.softmax(Z)\n",
    "                elif  active_function == \"relu\":\n",
    "                    A = DNNClassifier.ReLU(Z)\n",
    "                elif active_function == 'elu':\n",
    "                    A = DNNClassifier.elu(Z)\n",
    "            \n",
    "            if keep_prob != 1 and l != L and l != 1:\n",
    "                D = np.random.rand(A.shape[0],A.shape[1])\n",
    "                D = (D<keep_prob)\n",
    "                A = np.multiply(A,D)\n",
    "                A = A / keep_prob\n",
    "                cache = ((A_prev, parameters['W' + str(l)],parameters['b' + str(l)],D), Z)\n",
    "                caches.append(cache)\n",
    "            else:\n",
    "                cache = ((A_prev, parameters['W' + str(l)],parameters['b' + str(l)]), Z)\n",
    "                #print(A.shape)\n",
    "                caches.append(cache)      \n",
    "        return A, caches\n",
    "    @staticmethod\n",
    "    def compute_cost(A, Y, parameters, lamda=0,penality=None):\n",
    "        \"\"\"\n",
    "        Implement the cost function with L2 regularization. See formula (2) above.\n",
    "    \n",
    "        Arguments:\n",
    "            A -- post-activation, output of forward propagation\n",
    "            Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "            parameters -- python dictionary containing parameters of the model\n",
    "    \n",
    "        Returns:\n",
    "            cost - value of the regularized loss function \n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "    \n",
    "        cost = np.squeeze(-np.sum(np.multiply(np.log(A),Y))/m)\n",
    "    \n",
    "        L = len(parameters)//2\n",
    "    \n",
    "        if penality == 'l2' and lamda != 0:\n",
    "            sum_weights = 0\n",
    "            for l in range(1, L):\n",
    "                sum_weights = sum_weights + np.sum(np.square(parameters['W' + str(l)]))\n",
    "            cost = cost + sum_weights * (lamda/(2*m))\n",
    "        elif penality == 'l1' and lamda != 0:\n",
    "            sum_weights = 0\n",
    "            for l in range(1, L):\n",
    "                sum_weights = sum_weights + np.sum(np.abs(parameters['W' + str(l)]))\n",
    "            cost = cost + sum_weights * (lamda/(2*m))\n",
    "        return cost\n",
    "    @staticmethod\n",
    "    def back_propagation(AL, Y, caches, hidden_layers, keep_prob=1, penality=None,lamda=0):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation\n",
    "    \n",
    "        Arguments:\n",
    "            AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "            Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "            caches -- list of caches containing:\n",
    "            hidden_layers -- hidden layer names\n",
    "            keep_prob -- probabaility for dropout\n",
    "            penality -- regularization penality 'l1' or 'l2' or None\n",
    "    \n",
    "        Returns:\n",
    "             grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "        \"\"\"\n",
    "        grads = {}\n",
    "        L = len(caches) # the number of layers\n",
    "    \n",
    "        m = AL.shape[1]\n",
    "        Y = Y.reshape(AL.shape)\n",
    "    \n",
    "        # Initializing the backpropagation\n",
    "        dZL = AL - Y\n",
    "    \n",
    "        cache = caches[L-1]\n",
    "        linear_cache, activation_cache = cache\n",
    "        AL, W, b = linear_cache\n",
    "        grads[\"dW\" + str(L)] = np.dot(dZL,AL.T)/m\n",
    "        grads[\"db\" + str(L)] = np.sum(dZL,axis=1,keepdims=True)/m\n",
    "        grads[\"dA\" + str(L-1)] = np.dot(W.T,dZL)\n",
    "    \n",
    "    \n",
    "        # Loop from l=L-2 to l=0\n",
    "        v_dropout = 0\n",
    "        for l in reversed(range(L-1)):\n",
    "            cache = caches[l]\n",
    "            active_function = hidden_layers[l]\n",
    "        \n",
    "            linear_cache, Z = cache\n",
    "            try:\n",
    "                A_prev, W, b = linear_cache\n",
    "            except:\n",
    "                A_prev, W, b, D = linear_cache\n",
    "                v_dropout = 1\n",
    "            \n",
    "            m = A_prev.shape[1]\n",
    "        \n",
    "            if keep_prob != 1 and v_dropout == 1:\n",
    "                dA_prev = np.multiply(grads[\"dA\" + str(l + 1)],D)\n",
    "                dA_prev = dA_prev/keep_prob\n",
    "                v_dropout = 0\n",
    "            else:\n",
    "                dA_prev = grads[\"dA\" + str(l + 1)]\n",
    "                v_dropout = 0\n",
    "            \n",
    "            \n",
    "            if type(active_function) is tuple:\n",
    "                \n",
    "                if  active_function[0] == \"relu\":\n",
    "                    dZ = np.multiply(dA_prev,DNNClassifier.ReLU(Z,active_function[1],derivative=True))\n",
    "                elif active_function[0] == 'elu':\n",
    "                    dZ = np.multiply(dA_prev,DNNClassifier.elu(Z,active_function[1],derivative=True))\n",
    "            else:\n",
    "                if active_function == \"sigmoid\":\n",
    "                    dZ = np.multiply(dA_prev,DNNClassifier.sigmoid(Z,derivative=True))\n",
    "                elif active_function == \"relu\":\n",
    "                    dZ = np.multiply(dA_prev,DNNClassifier.ReLU(Z,derivative=True))\n",
    "                elif active_function == \"tanh\":\n",
    "                    dZ = np.multiply(dA_prev,DNNClassifier.Tanh(Z,derivative=True))\n",
    "                elif active_function == \"identity\":\n",
    "                    dZ = np.multiply(dA_prev,DNNClassifier.identity(Z,derivative=True))\n",
    "                elif active_function == \"arctan\":\n",
    "                    dZ = np.multiply(dA_prev,DNNClassifier.arctan(Z,derivative=True))\n",
    "                elif active_function == \"softplus\":\n",
    "                    dZ = np.multiply(dA_prev,DNNClassifier.softplus(Z,derivative=True))\n",
    "                elif active_function == 'elu':\n",
    "                    dZ = np.multiply(dA_prev,DNNClassifier.elu(Z,derivative=True))\n",
    "            \n",
    "            grads[\"dA\" + str(l)] = np.dot(W.T,dZ)\n",
    "        \n",
    "            if penality == 'l2':\n",
    "                grads[\"dW\" + str(l + 1)] = (np.dot(dZ,A_prev.T)/m)  + ((lamda * W)/m)\n",
    "            elif penality == 'l1':\n",
    "                grads[\"dW\" + str(l + 1)] = (np.dot(dZ,A_prev.T)/m)  + ((lamda * np.sign(W+10**-8))/m)\n",
    "            else:\n",
    "                grads[\"dW\" + str(l + 1)] = (np.dot(dZ,A_prev.T)/m)\n",
    "            \n",
    "            grads[\"db\" + str(l + 1)] = np.sum(dZ,axis=1,keepdims=True)/m   \n",
    "        return grads\n",
    "    \n",
    "    @staticmethod\n",
    "    def update_parameters(parameters, grads,learning_rate,iter_no,method = 'SGD',opt_parameters=None,beta1=0.9,beta2=0.999):\n",
    "        \"\"\"\n",
    "        Update parameters using gradient descent\n",
    "    \n",
    "        Arguments:\n",
    "        parameters -- python dictionary containing your parameters \n",
    "        grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "        method -- method for updation of weights\n",
    "                  'SGD','SGDM','RMSP','ADAM'\n",
    "        learning rate -- learning rate alpha value\n",
    "        beta1 -- weighted avg parameter for SGDM and ADAM\n",
    "        beta2 -- weighted avg parameter for RMSP and ADAM\n",
    "    \n",
    "        Returns:\n",
    "        parameters -- python dictionary containing your updated parameters \n",
    "                      parameters[\"W\" + str(l)] = ... \n",
    "                      parameters[\"b\" + str(l)] = ...\n",
    "                      opt_parameters\n",
    "        \"\"\"\n",
    "    \n",
    "        L = len(parameters) // 2 # number of layers in the neural network\n",
    "        if method == 'SGD':\n",
    "            for l in range(L):\n",
    "                parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l + 1)]\n",
    "                parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l + 1)]\n",
    "            opt_parameters = None\n",
    "        elif method == 'SGDM':\n",
    "            for l in range(L):\n",
    "                opt_parameters['vdb'+str(l+1)] = beta1*opt_parameters['vdb'+str(l+1)] + (1-beta1)*grads[\"db\" + str(l + 1)]\n",
    "                opt_parameters['vdw'+str(l+1)] = beta1*opt_parameters['vdw'+str(l+1)] + (1-beta1)*grads[\"dW\" + str(l + 1)]\n",
    "                parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*opt_parameters['vdw'+str(l+1)]\n",
    "                parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*opt_parameters['vdb'+str(l+1)]\n",
    "        elif method == 'RMSP':\n",
    "            for l in range(L):\n",
    "                opt_parameters['sdb'+str(l+1)] = beta2*opt_parameters['sdb'+str(l+1)] + \\\n",
    "                                                     (1-beta2)*np.square(grads[\"db\" + str(l + 1)])\n",
    "                opt_parameters['sdw'+str(l+1)] = beta2*opt_parameters['sdw'+str(l+1)] + \\\n",
    "                                                           (1-beta2)*np.square(grads[\"dW\" + str(l + 1)])\n",
    "                parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - \\\n",
    "                                       learning_rate*(grads[\"dW\" + str(l + 1)]/(np.sqrt(opt_parameters['sdw'+str(l+1)])+10**-8))\n",
    "                parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - \\\n",
    "                                       learning_rate*(grads[\"db\" + str(l + 1)]/(np.sqrt(opt_parameters['sdb'+str(l+1)])+10**-8))\n",
    "        elif method == 'ADAM':\n",
    "            for l in range(L):\n",
    "                opt_parameters['vdb'+str(l+1)] = beta1*opt_parameters['vdb'+str(l+1)] + (1-beta1)*grads[\"db\" + str(l + 1)]\n",
    "                opt_parameters['vdw'+str(l+1)] = beta1*opt_parameters['vdw'+str(l+1)] + (1-beta1)*grads[\"dW\" + str(l + 1)]\n",
    "                opt_parameters['sdb'+str(l+1)] = beta2*opt_parameters['sdb'+str(l+1)] + \\\n",
    "                                                                  (1-beta2)*np.square(grads[\"db\" + str(l + 1)])\n",
    "                opt_parameters['sdw'+str(l+1)] = beta2*opt_parameters['sdw'+str(l+1)] + \\\n",
    "                                                                   (1-beta2)*np.square(grads[\"dW\" + str(l + 1)])\n",
    "            \n",
    "                learning_rate = learning_rate * np.sqrt((1-beta2**iter_no)/((1-beta1**iter_no)+10**-8))\n",
    "                parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - \\\n",
    "                                       learning_rate*(opt_parameters['vdw'+str(l+1)]/\\\n",
    "                                                      (np.sqrt(opt_parameters['sdw'+str(l+1)])+10**-8))\n",
    "                parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - \\\n",
    "                                       learning_rate*(opt_parameters['vdb'+str(l+1)]/\\\n",
    "                                                      (np.sqrt(opt_parameters['sdb'+str(l+1)])+10**-8))\n",
    "        \n",
    "        return parameters,opt_parameters\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        '''\n",
    "        X -- data, numpy array of shape (input size, number of examples)\n",
    "        y -- lables, numpy array of shape (no of classes,n)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        np.random.seed(self.seed)\n",
    "        self.grads = {}\n",
    "        self.costs = []\n",
    "        M = X.shape[1]\n",
    "        opt_parameters = {}\n",
    "        \n",
    "        if self.verbose == 1:\n",
    "            print('Initilizing Weights...')\n",
    "        self.parameters = self.weights_init(self.layer_dims,self.init_type,self.seed)\n",
    "        self.iter_no = 0\n",
    "        idx = np.arange(0,M)\n",
    "        \n",
    "        if self.optimization_method != 'SGD':\n",
    "            for l in range(1, len(self.layer_dims)):\n",
    "                opt_parameters['vdw' + str(l)] = np.zeros((self.layer_dims[l], self.layer_dims[l-1]))\n",
    "                opt_parameters['vdb' + str(l)] = np.zeros((self.layer_dims[l], 1))\n",
    "                opt_parameters['sdw' + str(l)] = np.zeros((self.layer_dims[l], self.layer_dims[l-1]))\n",
    "                opt_parameters['sdb' + str(l)] = np.zeros((self.layer_dims[l], 1)) \n",
    "        \n",
    "        if self.verbose == 1:\n",
    "            print('Starting Training...')\n",
    "            \n",
    "        for epoch_no in range(1,self.max_epoch+1):\n",
    "            np.random.shuffle(idx)\n",
    "            X = X[:,idx]\n",
    "            y = y[:,idx]\n",
    "            for i in range(0,M, self.batch_size):\n",
    "                self.iter_no = self.iter_no + 1\n",
    "                X_batch = X[:,i:i + self.batch_size]\n",
    "                y_batch = y[:,i:i + self.batch_size]\n",
    "                # Forward propagation:\n",
    "                AL, cache = self.forward_propagation(X_batch,self.hidden_layers,self.parameters,self.keep_proba,self.seed)\n",
    "                #cost\n",
    "                cost = self.compute_cost(AL, y_batch, self.parameters,self.lamda,self.penality)\n",
    "                self.costs.append(cost)\n",
    "                \n",
    "                if self.tol != None:\n",
    "                    try:\n",
    "                        if abs(cost - self.costs[-2]) < self.tol:\n",
    "                            return self\n",
    "                    except:\n",
    "                        pass\n",
    "                #back prop\n",
    "                grads = self.back_propagation(AL, y_batch, cache,self.hidden_layers,self.keep_proba,self.penality,self.lamda)\n",
    "                \n",
    "                #update params\n",
    "                self.parameters,opt_parameters = self.update_parameters(self.parameters,grads,self.learning_rate,\n",
    "                                                                        self.iter_no-1,self.optimization_method,\n",
    "                                                                        opt_parameters,self.beta1,self.beta2)\n",
    "                \n",
    "                if self.verbose == 1:\n",
    "                    if self.iter_no % 100 == 0:\n",
    "                        print(\"Cost after iteration {}: {}\".format(self.iter_no, cost))\n",
    "                \n",
    "        return self\n",
    "    def predict(self,X,proba=False):\n",
    "        '''predicting values\n",
    "           arguments: X - iput data\n",
    "                      proba -- False then return value\n",
    "                               True then return probabaility\n",
    "        '''\n",
    "        \n",
    "        out, _ = self.forward_propagation(X,self.hidden_layers,self.parameters,self.keep_proba,self.seed)\n",
    "        if proba == True:\n",
    "            return out.T\n",
    "        else:\n",
    "            return np.argmax(out, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
