{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing NN class\n",
    "\n",
    "## Developing a code for doing neural networks with back propagation\n",
    "\n",
    "\n",
    "One can identify a set of key steps when using neural networks to solve supervised learning problems:  \n",
    "\n",
    "1. Collect and pre-process data  \n",
    "\n",
    "2. Define model and architecture  \n",
    "\n",
    "3. Choose cost function and optimizer  \n",
    "\n",
    "4. Train the model  \n",
    "\n",
    "5. Evaluate model performance on test data  \n",
    "\n",
    "6. Adjust hyperparameters (if necessary, network architecture)\n",
    "\n",
    "## Part b: Regression case\n",
    "\n",
    "The cost function is the Mean Square Error that is the one mainly used for the Regression case.\n",
    "The biases are initialized froma normal distribution ? \n",
    "There is no activation function of the output layer, because we are in the Regression case.\n",
    "\n",
    "### 1. Collect and pre-process data\n",
    "### 2. Train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(625,)\n",
      "(625, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import random, seed, shuffle\n",
    "from dataset import create_xyz_dataset, create_X, Plot_FrankeFunction\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from neuralnetwork import *\n",
    "from dnn import *\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "degree = 5\n",
    "n = 25\n",
    "mu_n = 0; sigma_n = 0.1 # Paramaters of noise distribution\n",
    "x,y,z = create_xyz_dataset(n,mu_n, sigma_n)\n",
    "# For debugging: Plot_FrankeFunction(x,y,z, title=\"Original dataset: : \\nFranke Function with stochastic noise\")\n",
    "Y = z.ravel(); n=n**2\n",
    "X= create_X(x,y)\n",
    "print(Y.shape)\n",
    "print(Y.reshape(n,1).shape)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define model and architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 2, 3, 1]\n",
      "(1, 625)\n",
      "1\n",
      "Wl.shape (3, 2)\n",
      "bl.shape (3, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (3,2) and (625,2) not aligned: 2 (dim 1) != 625 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d7042c2de127>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhidden_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mregrNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN_Regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mregrNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#regrDNN = DNNClassifier([2,3,2,3,1], ['sigmoid','sigmoid','sigmoid'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/MachineLearning/Projects/Project2/neuralnetwork.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, seed)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# 1) TRAIN THE NETWORK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/MachineLearning/Projects/Project2/neuralnetwork.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;31m# a) Forward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"feedforward works\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AL shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/MachineLearning/Projects/Project2/neuralnetwork.py\u001b[0m in \u001b[0;36mfeed_forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bl.shape\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0mZl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;31m#Evaluate the next layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (3,2) and (625,2) not aligned: 2 (dim 1) != 625 (dim 0)"
     ]
    }
   ],
   "source": [
    "hidden_layers = [3,2,3]\n",
    "regrNN = NN_Regression(hidden_layers)\n",
    "regrNN.fit(X,Y)\n",
    "\n",
    "#regrDNN = DNNClassifier([2,3,2,3,1], ['sigmoid','sigmoid','sigmoid'])\n",
    "#regrDNN.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.04166667  0.08333333 ...  0.91666667  0.95833333\n",
      "   1.        ]\n",
      " [ 0.          0.          0.         ...  1.          1.\n",
      "   1.        ]\n",
      " [ 0.81356411  0.67642776  0.96747555 ...  0.21811736  0.16997173\n",
      "  -0.00938617]]\n",
      "[[0.         0.04166667 0.08333333 ... 0.91666667 0.95833333 1.        ]\n",
      " [0.         0.04166667 0.08333333 ... 0.91666667 0.95833333 1.        ]\n",
      " [0.         0.         0.         ... 1.         1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "training_data = np.vstack((x.ravel(),y.ravel(), z.ravel()))\n",
    "print(training_data[:3])\n",
    "shuffle(training_data)\n",
    "print(training_data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "\n",
    "*Train your network and compare the results with those from your OLS and Ridge Regression codes from project 1. \n",
    "You should test your results against a similar code using **Scikit-Learn** (see the examples in the above lecture notes from week 41) or **tensorflow/keras**.*\n",
    "\n",
    "Comment your results and give a critical discussion of the results\n",
    "obtained with the Linear  Regression code and your own Neural Network\n",
    "code.  Compare the results with those from project 1.\n",
    "Make an analysis of the regularization parameters and the learning rates employed to find the optimal MSE and $R2$ scores.\n",
    "\n",
    "A useful reference on the back progagation algorithm is [Nielsen's\n",
    "book](http://neuralnetworksanddeeplearning.com/). It is an excellent\n",
    "read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNClassifier(object):\n",
    "    '''\n",
    "    Parameters: layer_dims -- List Dimensions of layers including input and output layer\n",
    "                hidden_layers -- List of hidden layers\n",
    "                                 'relu','sigmoid','tanh','softplus','arctan','elu','identity','softmax'\n",
    "                                 Note: 1. last layer must be softmax \n",
    "                                       2. For relu and elu need to mention alpha value as below\n",
    "                                        ['tanh',('relu',alpha1),('elu',alpha2),('relu',alpha3),'softmax']\n",
    "                                        need to give a tuple for relu and elu if you want to mention alpha\n",
    "                                        if not default alpha is 0\n",
    "                init_type -- init_type -- he_normal  --> N(0,sqrt(2/fanin))\n",
    "                             he_uniform --> Uniform(-sqrt(6/fanin),sqrt(6/fanin))\n",
    "                             xavier_normal --> N(0,2/(fanin+fanout))\n",
    "                             xavier_uniform --> Uniform(-sqrt(6/fanin+fanout),sqrt(6/fanin+fanout))\n",
    "                                 \n",
    "                learning_rate -- Learning rate\n",
    "                optimization_method -- optimization method 'SGD','SGDM','RMSP','ADAM'\n",
    "                batch_size -- Batch size to update weights \n",
    "                max_epoch -- Max epoch number\n",
    "                             Note : Max_iter  = max_epoch * (size of traing / batch size)\n",
    "                tolarance -- if abs(previous cost  - current cost ) < tol training will be stopped\n",
    "                             if None -- No check will be performed\n",
    "                keep_proba -- probability for dropout\n",
    "                              if 1 then there is no dropout\n",
    "                penality -- regularization penality\n",
    "                            values taken 'l1','l2',None(default)\n",
    "                lamda -- l1 or l2 regularization value\n",
    "                beta1 -- SGDM and adam optimization param\n",
    "                beta2 -- RMSP and adam optimization value\n",
    "                seed -- Random seed to generate randomness\n",
    "                verbose -- takes 0  or 1 \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,layer_dims,hidden_layers,init_type='he_normal',learning_rate=0.1,\n",
    "                 optimization_method = 'SGD',batch_size=64,max_epoch=100,tolarance = 0.00001,\n",
    "                 keep_proba=1,penality=None,lamda=0,beta1=0.9,\n",
    "                 beta2=0.999,seed=None,verbose=0):\n",
    "        self.layer_dims = layer_dims\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.init_type = init_type\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimization_method = optimization_method\n",
    "        self.batch_size = batch_size\n",
    "        self.keep_proba = keep_proba\n",
    "        self.penality = penality\n",
    "        self.lamda = lamda\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.seed = seed\n",
    "        self.max_epoch = max_epoch\n",
    "        self.tol = tolarance\n",
    "        self.verbose = verbose\n",
    "    @staticmethod\n",
    "    def weights_init(layer_dims,init_type='he_normal',seed=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "            layer_dims lis is like  [ no of input features,# of neurons in hidden layer-1,..,\n",
    "                                     # of neurons in hidden layer-n shape,output]\n",
    "            init_type -- he_normal  --> N(0,sqrt(2/fanin))\n",
    "                         he_uniform --> Uniform(-sqrt(6/fanin),sqrt(6/fanin))\n",
    "                         xavier_normal --> N(0,2/(fanin+fanout))\n",
    "                         xavier_uniform --> Uniform(-sqrt(6/fanin+fanout),sqrt(6/fanin+fanout))\n",
    "                         seed -- random seed to generate weights\n",
    "        Returns:\n",
    "            parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        parameters = {}\n",
    "        opt_parameters = {}\n",
    "        L = len(layer_dims)            # number of layers in the network\n",
    "        if  init_type == 'he_normal':\n",
    "            for l in range(1, L):\n",
    "                parameters['W' + str(l)] = np.random.normal(0,np.sqrt(2.0/layer_dims[l-1]),(layer_dims[l], layer_dims[l-1]))\n",
    "                parameters['b' + str(l)] = np.random.normal(0,np.sqrt(2.0/layer_dims[l-1]),(layer_dims[l], 1))  \n",
    "            \n",
    "        elif init_type == 'he_uniform':\n",
    "            for l in range(1, L):\n",
    "                parameters['W' + str(l)] = np.random.uniform(-np.sqrt(6.0/layer_dims[l-1]),\n",
    "                                                        np.sqrt(6.0/layer_dims[l-1]),\n",
    "                                                        (layer_dims[l], layer_dims[l-1]))\n",
    "                parameters['b' + str(l)] = np.random.uniform(-np.sqrt(6.0/layer_dims[l-1]),\n",
    "                                                        np.sqrt(6.0/layer_dims[l-1]),\n",
    "                                                        (layer_dims[l], 1))\n",
    "            \n",
    "        elif init_type == 'xavier_normal':\n",
    "            for l in range(1, L):\n",
    "                parameters['W' + str(l)] = np.random.normal(0,2.0/(layer_dims[l]+layer_dims[l-1]),\n",
    "                                                                   (layer_dims[l], layer_dims[l-1]))\n",
    "                parameters['b' + str(l)] = np.random.normal(0,2.0/(layer_dims[l]+layer_dims[l-1]),\n",
    "                                                                      (layer_dims[l], 1)) \n",
    "            \n",
    "        elif init_type == 'xavier_uniform':\n",
    "            for l in range(1, L):\n",
    "                parameters['W' + str(l)] = np.random.uniform(-(np.sqrt(6.0/(layer_dims[l]+layer_dims[l-1]))),\n",
    "                                                        (np.sqrt(6.0/(layer_dims[l]+layer_dims[l-1]))),\n",
    "                                                        (layer_dims[l], layer_dims[l-1]))\n",
    "                parameters['b' + str(l)] = np.random.uniform(-(np.sqrt(6.0/(layer_dims[l]+layer_dims[l-1]))),\n",
    "                                                        (np.sqrt(6.0/(layer_dims[l]+layer_dims[l-1]))),\n",
    "                                                        (layer_dims[l], 1))\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(X,derivative=False):\n",
    "        '''Compute Sigmaoid and its derivative'''\n",
    "        if derivative == False:\n",
    "            out = 1 / (1 + np.exp(-np.array(X)))\n",
    "        elif derivative == True:\n",
    "            s = 1 / (1 + np.exp(-np.array(X)))\n",
    "            out = s*(1-s)\n",
    "        return out\n",
    "    @staticmethod\n",
    "    def ReLU(X,alpha=0,derivative=False):\n",
    "        '''Compute ReLU function and derivative'''\n",
    "        X = np.array(X,dtype=np.float64)\n",
    "        if derivative == False:\n",
    "            return np.where(X<0,alpha*X,X)\n",
    "        elif derivative == True:\n",
    "            X_relu = np.ones_like(X,dtype=np.float64)\n",
    "            X_relu[X < 0] = alpha\n",
    "            return X_relu\n",
    "    @staticmethod\n",
    "    def Tanh(X,derivative=False):\n",
    "        '''Compute tanh values and derivative of tanh'''\n",
    "        X = np.array(X)\n",
    "        if derivative == False:\n",
    "            return np.tanh(X)\n",
    "        if derivative == True:\n",
    "            return 1 - (np.tanh(X))**2\n",
    "    @staticmethod\n",
    "    def softplus(X,derivative=False):\n",
    "        '''Compute tanh values and derivative of tanh'''\n",
    "        X = np.array(X)\n",
    "        if derivative == False:\n",
    "            return np.log(1+np.exp(X))\n",
    "        if derivative == True:\n",
    "            return 1 / (1 + np.exp(-np.array(X)))\n",
    "    @staticmethod\n",
    "    def arctan(X,derivative=False):\n",
    "        '''Compute tan^-1(X) and derivative'''\n",
    "        if derivative == False:\n",
    "            return  np.arctan(X)\n",
    "        if derivative == True:\n",
    "            return 1/ (1 + np.square(X))\n",
    "    @staticmethod\n",
    "    def identity(X,derivative=False):\n",
    "        '''identity function and derivative f(x) = x'''\n",
    "        X = np.array(X)\n",
    "        if derivative ==  False:\n",
    "            return X\n",
    "        if derivative == True:\n",
    "            return np.ones_like(X)\n",
    "    @staticmethod\n",
    "    def elu(X,alpha=0,derivative=False):\n",
    "        '''Exponential Linear Unit'''\n",
    "        X = np.array(X,dtype=np.float64)\n",
    "        if derivative == False:\n",
    "            return np.where(X<0,alpha*(np.exp(X)-1),X)\n",
    "        elif derivative == True:\n",
    "            return np.where(X<0,alpha*(np.exp(X)),1)\n",
    "    @staticmethod\n",
    "    def softmax(X):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        return np.exp(X) / np.sum(np.exp(X),axis=0)\n",
    "    @staticmethod\n",
    "    def forward_propagation(X, hidden_layers,parameters,keep_prob=1,seed=None):\n",
    "    \n",
    "        \"\"\"\"\n",
    "        Arguments:\n",
    "            X -- data, numpy array of shape (input size, number of examples)\n",
    "            hidden_layers -- List of hideden layers\n",
    "            weights -- Output of weights_init dict (parameters)\n",
    "            keep_prob -- probability of keeping a neuron active during drop-out, scalar\n",
    "        Returns:\n",
    "            AL -- last post-activation value\n",
    "            caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "        \"\"\"\n",
    "        if seed != None:\n",
    "            np.random.seed(seed)\n",
    "        caches = []\n",
    "        A = X\n",
    "        L = len(hidden_layers)\n",
    "        for l,active_function in enumerate(hidden_layers,start=1):\n",
    "            A_prev = A \n",
    "        \n",
    "            Z = np.dot(parameters['W' + str(l)],A_prev)+parameters['b' + str(l)]\n",
    "            \n",
    "            if type(active_function) is tuple:\n",
    "                \n",
    "                if  active_function[0] == \"relu\":\n",
    "                    A = DNNClassifier.ReLU(Z,active_function[1])\n",
    "                elif active_function[0] == 'elu':\n",
    "                    A = DNNClassifier.elu(Z,active_function[1])\n",
    "            else:\n",
    "                if active_function == \"sigmoid\":\n",
    "                    A = DNNClassifier.sigmoid(Z)\n",
    "                elif active_function == \"identity\":\n",
    "                    A = DNNClassifier.identity(Z)\n",
    "                elif active_function == \"arctan\":\n",
    "                    A = DNNClassifier.arctan(Z)\n",
    "                elif active_function == \"softplus\":\n",
    "                    A = DNNClassifier.softplus(Z)\n",
    "                elif active_function == \"tanh\":\n",
    "                    A = DNNClassifier.Tanh(Z)\n",
    "                elif active_function == \"softmax\":\n",
    "                    A = DNNClassifier.softmax(Z)\n",
    "                elif  active_function == \"relu\":\n",
    "                    A = DNNClassifier.ReLU(Z)\n",
    "                elif active_function == 'elu':\n",
    "                    A = DNNClassifier.elu(Z)\n",
    "            \n",
    "            if keep_prob != 1 and l != L and l != 1:\n",
    "                D = np.random.rand(A.shape[0],A.shape[1])\n",
    "                D = (D<keep_prob)\n",
    "                A = np.multiply(A,D)\n",
    "                A = A / keep_prob\n",
    "                cache = ((A_prev, parameters['W' + str(l)],parameters['b' + str(l)],D), Z)\n",
    "                caches.append(cache)\n",
    "            else:\n",
    "                cache = ((A_prev, parameters['W' + str(l)],parameters['b' + str(l)]), Z)\n",
    "                #print(A.shape)\n",
    "                caches.append(cache)      \n",
    "        return A, caches\n",
    "    @staticmethod\n",
    "    def compute_cost(A, Y, parameters, lamda=0,penality=None):\n",
    "        \"\"\"\n",
    "        Implement the cost function with L2 regularization. See formula (2) above.\n",
    "    \n",
    "        Arguments:\n",
    "            A -- post-activation, output of forward propagation\n",
    "            Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "            parameters -- python dictionary containing parameters of the model\n",
    "    \n",
    "        Returns:\n",
    "            cost - value of the regularized loss function \n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "    \n",
    "        cost = np.squeeze(-np.sum(np.multiply(np.log(A),Y))/m)\n",
    "    \n",
    "        L = len(parameters)//2\n",
    "    \n",
    "        if penality == 'l2' and lamda != 0:\n",
    "            sum_weights = 0\n",
    "            for l in range(1, L):\n",
    "                sum_weights = sum_weights + np.sum(np.square(parameters['W' + str(l)]))\n",
    "            cost = cost + sum_weights * (lamda/(2*m))\n",
    "        elif penality == 'l1' and lamda != 0:\n",
    "            sum_weights = 0\n",
    "            for l in range(1, L):\n",
    "                sum_weights = sum_weights + np.sum(np.abs(parameters['W' + str(l)]))\n",
    "            cost = cost + sum_weights * (lamda/(2*m))\n",
    "        return cost\n",
    "    @staticmethod\n",
    "    def back_propagation(AL, Y, caches, hidden_layers, keep_prob=1, penality=None,lamda=0):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation\n",
    "    \n",
    "        Arguments:\n",
    "            AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "            Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "            caches -- list of caches containing:\n",
    "            hidden_layers -- hidden layer names\n",
    "            keep_prob -- probabaility for dropout\n",
    "            penality -- regularization penality 'l1' or 'l2' or None\n",
    "    \n",
    "        Returns:\n",
    "             grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "        \"\"\"\n",
    "        grads = {}\n",
    "        L = len(caches) # the number of layers\n",
    "    \n",
    "        m = AL.shape[1]\n",
    "        Y = Y.reshape(AL.shape)\n",
    "    \n",
    "        # Initializing the backpropagation\n",
    "        dZL = AL - Y\n",
    "    \n",
    "        cache = caches[L-1]\n",
    "        linear_cache, activation_cache = cache\n",
    "        AL, W, b = linear_cache\n",
    "        grads[\"dW\" + str(L)] = np.dot(dZL,AL.T)/m\n",
    "        grads[\"db\" + str(L)] = np.sum(dZL,axis=1,keepdims=True)/m\n",
    "        grads[\"dA\" + str(L-1)] = np.dot(W.T,dZL)\n",
    "    \n",
    "    \n",
    "        # Loop from l=L-2 to l=0\n",
    "        v_dropout = 0\n",
    "        for l in reversed(range(L-1)):\n",
    "            cache = caches[l]\n",
    "            active_function = hidden_layers[l]\n",
    "        \n",
    "            linear_cache, Z = cache\n",
    "            try:\n",
    "                A_prev, W, b = linear_cache\n",
    "            except:\n",
    "                A_prev, W, b, D = linear_cache\n",
    "                v_dropout = 1\n",
    "            \n",
    "            m = A_prev.shape[1]\n",
    "        \n",
    "            if keep_prob != 1 and v_dropout == 1:\n",
    "                dA_prev = np.multiply(grads[\"dA\" + str(l + 1)],D)\n",
    "                dA_prev = dA_prev/keep_prob\n",
    "                v_dropout = 0\n",
    "            else:\n",
    "                dA_prev = grads[\"dA\" + str(l + 1)]\n",
    "                v_dropout = 0\n",
    "            \n",
    "            \n",
    "            if type(active_function) is tuple:\n",
    "                \n",
    "                if  active_function[0] == \"relu\":\n",
    "                    dZ = np.multiply(dA_prev,DNNClassifier.ReLU(Z,active_function[1],derivative=True))\n",
    "                elif active_function[0] == 'elu':\n",
    "                    dZ = np.multiply(dA_prev,DNNClassifier.elu(Z,active_function[1],derivative=True))\n",
    "            else:\n",
    "                if active_function == \"sigmoid\":\n",
    "                    dZ = np.multiply(dA_prev,DNNClassifier.sigmoid(Z,derivative=True))\n",
    "                elif active_function == \"relu\":\n",
    "                    dZ = np.multiply(dA_prev,DNNClassifier.ReLU(Z,derivative=True))\n",
    "                elif active_function == \"tanh\":\n",
    "                    dZ = np.multiply(dA_prev,DNNClassifier.Tanh(Z,derivative=True))\n",
    "                elif active_function == \"identity\":\n",
    "                    dZ = np.multiply(dA_prev,DNNClassifier.identity(Z,derivative=True))\n",
    "                elif active_function == \"arctan\":\n",
    "                    dZ = np.multiply(dA_prev,DNNClassifier.arctan(Z,derivative=True))\n",
    "                elif active_function == \"softplus\":\n",
    "                    dZ = np.multiply(dA_prev,DNNClassifier.softplus(Z,derivative=True))\n",
    "                elif active_function == 'elu':\n",
    "                    dZ = np.multiply(dA_prev,DNNClassifier.elu(Z,derivative=True))\n",
    "            \n",
    "            grads[\"dA\" + str(l)] = np.dot(W.T,dZ)\n",
    "        \n",
    "            if penality == 'l2':\n",
    "                grads[\"dW\" + str(l + 1)] = (np.dot(dZ,A_prev.T)/m)  + ((lamda * W)/m)\n",
    "            elif penality == 'l1':\n",
    "                grads[\"dW\" + str(l + 1)] = (np.dot(dZ,A_prev.T)/m)  + ((lamda * np.sign(W+10**-8))/m)\n",
    "            else:\n",
    "                grads[\"dW\" + str(l + 1)] = (np.dot(dZ,A_prev.T)/m)\n",
    "            \n",
    "            grads[\"db\" + str(l + 1)] = np.sum(dZ,axis=1,keepdims=True)/m   \n",
    "        return grads\n",
    "    \n",
    "    @staticmethod\n",
    "    def update_parameters(parameters, grads,learning_rate,iter_no,method = 'SGD',opt_parameters=None,beta1=0.9,beta2=0.999):\n",
    "        \"\"\"\n",
    "        Update parameters using gradient descent\n",
    "    \n",
    "        Arguments:\n",
    "        parameters -- python dictionary containing your parameters \n",
    "        grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "        method -- method for updation of weights\n",
    "                  'SGD','SGDM','RMSP','ADAM'\n",
    "        learning rate -- learning rate alpha value\n",
    "        beta1 -- weighted avg parameter for SGDM and ADAM\n",
    "        beta2 -- weighted avg parameter for RMSP and ADAM\n",
    "    \n",
    "        Returns:\n",
    "        parameters -- python dictionary containing your updated parameters \n",
    "                      parameters[\"W\" + str(l)] = ... \n",
    "                      parameters[\"b\" + str(l)] = ...\n",
    "                      opt_parameters\n",
    "        \"\"\"\n",
    "    \n",
    "        L = len(parameters) // 2 # number of layers in the neural network\n",
    "        if method == 'SGD':\n",
    "            for l in range(L):\n",
    "                parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l + 1)]\n",
    "                parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l + 1)]\n",
    "            opt_parameters = None\n",
    "        elif method == 'SGDM':\n",
    "            for l in range(L):\n",
    "                opt_parameters['vdb'+str(l+1)] = beta1*opt_parameters['vdb'+str(l+1)] + (1-beta1)*grads[\"db\" + str(l + 1)]\n",
    "                opt_parameters['vdw'+str(l+1)] = beta1*opt_parameters['vdw'+str(l+1)] + (1-beta1)*grads[\"dW\" + str(l + 1)]\n",
    "                parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*opt_parameters['vdw'+str(l+1)]\n",
    "                parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*opt_parameters['vdb'+str(l+1)]\n",
    "        elif method == 'RMSP':\n",
    "            for l in range(L):\n",
    "                opt_parameters['sdb'+str(l+1)] = beta2*opt_parameters['sdb'+str(l+1)] + \\\n",
    "                                                     (1-beta2)*np.square(grads[\"db\" + str(l + 1)])\n",
    "                opt_parameters['sdw'+str(l+1)] = beta2*opt_parameters['sdw'+str(l+1)] + \\\n",
    "                                                           (1-beta2)*np.square(grads[\"dW\" + str(l + 1)])\n",
    "                parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - \\\n",
    "                                       learning_rate*(grads[\"dW\" + str(l + 1)]/(np.sqrt(opt_parameters['sdw'+str(l+1)])+10**-8))\n",
    "                parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - \\\n",
    "                                       learning_rate*(grads[\"db\" + str(l + 1)]/(np.sqrt(opt_parameters['sdb'+str(l+1)])+10**-8))\n",
    "        elif method == 'ADAM':\n",
    "            for l in range(L):\n",
    "                opt_parameters['vdb'+str(l+1)] = beta1*opt_parameters['vdb'+str(l+1)] + (1-beta1)*grads[\"db\" + str(l + 1)]\n",
    "                opt_parameters['vdw'+str(l+1)] = beta1*opt_parameters['vdw'+str(l+1)] + (1-beta1)*grads[\"dW\" + str(l + 1)]\n",
    "                opt_parameters['sdb'+str(l+1)] = beta2*opt_parameters['sdb'+str(l+1)] + \\\n",
    "                                                                  (1-beta2)*np.square(grads[\"db\" + str(l + 1)])\n",
    "                opt_parameters['sdw'+str(l+1)] = beta2*opt_parameters['sdw'+str(l+1)] + \\\n",
    "                                                                   (1-beta2)*np.square(grads[\"dW\" + str(l + 1)])\n",
    "            \n",
    "                learning_rate = learning_rate * np.sqrt((1-beta2**iter_no)/((1-beta1**iter_no)+10**-8))\n",
    "                parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - \\\n",
    "                                       learning_rate*(opt_parameters['vdw'+str(l+1)]/\\\n",
    "                                                      (np.sqrt(opt_parameters['sdw'+str(l+1)])+10**-8))\n",
    "                parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - \\\n",
    "                                       learning_rate*(opt_parameters['vdb'+str(l+1)]/\\\n",
    "                                                      (np.sqrt(opt_parameters['sdb'+str(l+1)])+10**-8))\n",
    "        \n",
    "        return parameters,opt_parameters\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        '''\n",
    "        X -- data, numpy array of shape (input size, number of examples)\n",
    "        y -- lables, numpy array of shape (no of classes,n)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        np.random.seed(self.seed)\n",
    "        self.grads = {}\n",
    "        self.costs = []\n",
    "        M = X.shape[1]\n",
    "        opt_parameters = {}\n",
    "        \n",
    "        if self.verbose == 1:\n",
    "            print('Initilizing Weights...')\n",
    "        self.parameters = self.weights_init(self.layer_dims,self.init_type,self.seed)\n",
    "        self.iter_no = 0\n",
    "        idx = np.arange(0,M)\n",
    "        \n",
    "        if self.optimization_method != 'SGD':\n",
    "            for l in range(1, len(self.layer_dims)):\n",
    "                opt_parameters['vdw' + str(l)] = np.zeros((self.layer_dims[l], self.layer_dims[l-1]))\n",
    "                opt_parameters['vdb' + str(l)] = np.zeros((self.layer_dims[l], 1))\n",
    "                opt_parameters['sdw' + str(l)] = np.zeros((self.layer_dims[l], self.layer_dims[l-1]))\n",
    "                opt_parameters['sdb' + str(l)] = np.zeros((self.layer_dims[l], 1)) \n",
    "        \n",
    "        if self.verbose == 1:\n",
    "            print('Starting Training...')\n",
    "            \n",
    "        for epoch_no in range(1,self.max_epoch+1):\n",
    "            np.random.shuffle(idx)\n",
    "            X = X[:,idx]\n",
    "            y = y[:,idx]\n",
    "            for i in range(0,M, self.batch_size):\n",
    "                self.iter_no = self.iter_no + 1\n",
    "                X_batch = X[:,i:i + self.batch_size]\n",
    "                y_batch = y[:,i:i + self.batch_size]\n",
    "                # Forward propagation:\n",
    "                AL, cache = self.forward_propagation(X_batch,self.hidden_layers,self.parameters,self.keep_proba,self.seed)\n",
    "                #cost\n",
    "                cost = self.compute_cost(AL, y_batch, self.parameters,self.lamda,self.penality)\n",
    "                self.costs.append(cost)\n",
    "                \n",
    "                if self.tol != None:\n",
    "                    try:\n",
    "                        if abs(cost - self.costs[-2]) < self.tol:\n",
    "                            return self\n",
    "                    except:\n",
    "                        pass\n",
    "                #back prop\n",
    "                grads = self.back_propagation(AL, y_batch, cache,self.hidden_layers,self.keep_proba,self.penality,self.lamda)\n",
    "                \n",
    "                #update params\n",
    "                self.parameters,opt_parameters = self.update_parameters(self.parameters,grads,self.learning_rate,\n",
    "                                                                        self.iter_no-1,self.optimization_method,\n",
    "                                                                        opt_parameters,self.beta1,self.beta2)\n",
    "                \n",
    "                if self.verbose == 1:\n",
    "                    if self.iter_no % 100 == 0:\n",
    "                        print(\"Cost after iteration {}: {}\".format(self.iter_no, cost))\n",
    "                \n",
    "        return self\n",
    "    def predict(self,X,proba=False):\n",
    "        '''predicting values\n",
    "           arguments: X - iput data\n",
    "                      proba -- False then return value\n",
    "                               True then return probabaility\n",
    "        '''\n",
    "        \n",
    "        out, _ = self.forward_propagation(X,self.hidden_layers,self.parameters,self.keep_proba,self.seed)\n",
    "        if proba == True:\n",
    "            return out.T\n",
    "        else:\n",
    "            return np.argmax(out, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part ?\n",
    "\n",
    "The cost function is the Cross Entropy that is the one mainly used for the Classification case.\n",
    "The activation function of the output layer is the 'softmax', because we are in the Classification case.\n",
    "\n",
    "### 1. Collect and pre-process data\n",
    "Look at week 41"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
