{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "https://www.youtube.com/watch?v=Ilg3gGewQ5U\n",
    "\n",
    "The Back propagation is the core algorithm behind how neural networks learn. Learning means that the NN is trained to find which weights and biases minimize the cost function (e.g. MSE for Linear Regression, Cross-Entropy for Logistic Regression). We use SGD (e simili) that evalutates the negative gradient and correct the result iteration by iteration. Since we are in a dense netwrok and we need to correct all the weights and biases, backpropagation is an algorithm for computing this complicated gardient. \n",
    "\n",
    "The basic idea is starting from the output layer $O$, comparing it to the desired output $Y$ thanks to the cost function $C(O,Y)$. The cost function tells us how much each outputs in $O$ is far away from the expected ones and so how much they need to corrected. But how can we do it? The three actors of that result are: \n",
    "- weights $W_L$\n",
    "- bias $b_L$\n",
    "- activation function $a_L$\n",
    "Since the activation function is part of the architecture of the network we don't have a direct influence in chnage it, while we can \"weight\" differently the activation outputs.\n",
    "Given that we have basically a list of nudges that propagates backward in the network, because each layer depends on the previous one.\n",
    "\n",
    "Mathematically specking, the propagation algorithm is nothing else than the chain rule.\n",
    "https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html\n",
    "\n",
    "\n",
    "Then every other exaples (digit 1, 4, 5...) --> mean for each delta wi to get the whole negative gradient\n",
    "\n",
    "Negative gradient quantifies how you need to change all of the weights and biases, so as to most efficientlt decrease the cost function\n",
    "\n",
    "SGD: it is more efficient than GD --> work on smaller minibatches (compared one another) and than compare the results\n",
    "\n",
    "784 --> raveled\n",
    "\n",
    "\n",
    "Backpropagation: algorithm for determining how a single training example would like to nudge the weights and biases, in terms of waht relative proportions to those changes cause the most rapid decrease to the cost fucntion.\n",
    "\n",
    "GD: would involve doing this for all the tens and thousands of training examples and averaging the desired changes that you get.\n",
    "SGD: we randmly subdivide the data into these mini-batches and compute each step with respect to a mini-batch. Repeatedly going through all the mini batches and making these adjustments, you will converge to a local minimum of the cost function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
