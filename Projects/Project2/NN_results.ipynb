{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing NN class\n",
    "\n",
    "## Developing a code for doing neural networks with back propagation\n",
    "\n",
    "\n",
    "One can identify a set of key steps when using neural networks to solve supervised learning problems:  \n",
    "\n",
    "1. Collect and pre-process data  \n",
    "\n",
    "2. Define model and architecture  \n",
    "\n",
    "3. Choose cost function and optimizer  \n",
    "\n",
    "4. Train the model  \n",
    "\n",
    "5. Evaluate model performance on test data  \n",
    "\n",
    "6. Adjust hyperparameters (if necessary, network architecture)\n",
    "\n",
    "## Part b: Regression case\n",
    "\n",
    "The cost function is the Mean Square Error that is the one mainly used for the Regression case.\n",
    "The biases are initialized froma normal distribution ? \n",
    "There is no activation function of the output layer, because we are in the Regression case.\n",
    "\n",
    "### 1. Collect and pre-process data\n",
    "### 2. Train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random, seed\n",
    "from dataset import create_xyz_dataset, create_X, Plot_FrankeFunction\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "n = 25\n",
    "mu_n = 0; sigma_n = 0.1 # Paramaters of noise distribution\n",
    "x,y,z = create_xyz_dataset(n,mu_n, sigma_n)\n",
    "# For debugging: Plot_FrankeFunction(x,y,z, title=\"Original dataset: : \\nFranke Function with stochastic noise\")\n",
    "Y = z.ravel(); n=n**2\n",
    "X = create_X(x,y)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adelezaini/Desktop/MachineLearning/Projects/Project2/neuralnetwork.py:88: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(len(hidden_activations) == len(self.n_hidden_layers), \"Lenght of 'hidden_activations' list doesn't match with 'hidden_layer' lenght.\")\n",
      "/Users/adelezaini/Desktop/MachineLearning/Projects/Project2/neuralnetwork.py:119: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(X.shape[0] == Y.shape[0], \"Size of input is different of size of the output\")\n",
      "/Users/adelezaini/Desktop/MachineLearning/Projects/Project2/neuralnetwork.py:356: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(eta > eta_opt, \"Learning rate higher than the inverse of the max eigenvalue of the Hessian matrix: SGD will not converge to the minimum. Need to set another learning rate or its paramentes.\")\n",
      "/Users/adelezaini/Desktop/MachineLearning/Projects/Project2/algorithm.py:139: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(eta > eta_opt, \"Learning rate higher than the inverse of the max eigenvalue of the Hessian matrix: SGD will not converge to the minimum. Need to set another learning rate or its paramentes.\")\n"
     ]
    }
   ],
   "source": [
    "from neuralnetwork import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part ?\n",
    "\n",
    "The cost function is the Cross Entropy that is the one mainly used for the Classification case.\n",
    "The activation function of the output layer is the 'softmax', because we are in the Classification case.\n",
    "\n",
    "### 1. Collect and pre-process data\n",
    "Look at week 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MIT License (MIT)\n",
    "#\n",
    "# Copyright © 2021 Adele Zaini\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n",
    "# documentation files (the “Software”), to deal in the Software without restriction, including without limitation the\n",
    "# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,\n",
    "# and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all copies or substantial portions of\n",
    "# the Software. THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT\n",
    "# LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n",
    "# SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF\n",
    "# CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n",
    "# IN THE SOFTWARE.\n",
    "\n",
    "import numpy as np\n",
    "from random import random, seed\n",
    "from algorithm import SGD\n",
    "from activation import sigmoid, tanh, relu, leaky_relu, elu, softmax\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from algorithm import gradient_of\n",
    "\n",
    "#activations = ['sigmoid', 'tanh', 'relu', 'leaky_relu', 'elu', 'softmax']\n",
    "ACTIVATIONS = {'sigmoid': sigmoid, 'tanh': tanh, 'relu': relu, 'leaky_relu': leaky_relu, 'elu': elu, 'softmax': softmax, None: None}\n",
    "OPTIMIZERS = ['SGD', 'ADAGRAD', 'RMS', 'ADAM', 'SGDM']\n",
    "ETAS = ['static', 'schedule', 'invscaling', 'hessian']\n",
    "DELTA = 1e-8\n",
    "\n",
    "    \n",
    "# cost function\n",
    "# look at here : for cross entropy https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html\n",
    "# derivative of cost function\n",
    "# predict -> logistic or linear . look at :https://github.com/UdiBhaskar/Deep-Learning/blob/master/DNN%20in%20python%20from%20scratch.ipynb\n",
    "\n",
    "class NeuralNetwork:\n",
    "      \"\"\"This class creates a Feed Forward Neural Network with the backpropagation algorithm.\n",
    "      \n",
    "      Class members after initializing and fitting the class:\n",
    "        - n_inputs (int): number of inputs (i.e. number of rows of X and Y)\n",
    "        - n_features (int): number of feutures/examples (i.e. number of columns of X)\n",
    "        - n_categories (int): number of categories (i.e. number of columns of Y)\n",
    "        - hidden_layers_dims (int list): list of number of neurons for each hidden (!) layer (e.g. [2,3,4,2])\n",
    "        - n_hidden_layers (int): number of hidden layer\n",
    "        - layers_dims (int list): list of number of neurons for each layer (e.g. [n_feutures, 2,3,4,2, n_categories])\n",
    "        - activations_list (string list): list of activation functions name of the hidden layers + output activation function (default: None). The output activation function chagnes in changing the derivative class to NN_Classifier ('softmax')\n",
    "        - parameters (dict): optimized weights and biases (after trained the NN)\n",
    "        - grads (dict): gradients (after back propagation)\n",
    "        - costs (list): list of cost results step by step in the optimization process\n",
    "        - alpha (float): parameter for 'elu' and 'leaky_relu' activation function\n",
    "        - opt (string): optimizer ['SGD', 'ADAGRAD', 'RMS', 'ADAM', 'SGDM']\n",
    "        - batch_size (int): batch size for the SGD\n",
    "        - n_epochs (int): number of epochs for the SGD\n",
    "        - b1, b2: parameters to set up the optimization algorithm\n",
    "        - eta0 (float): basic parameter for many learning rate schedule\n",
    "        - eta_type (string): name of the choosen learning rate schedule\n",
    "        - t1 (float): paramater needed for the learning rate 'schedule'\n",
    "        - penality, lmd: parameters to set the regularization routine\n",
    "      \"\"\"\n",
    "\n",
    "    \n",
    "########## INITIALIZE NEURAL NETWORK\n",
    "      def __init__(self, hidden_layes_dims, hidden_activations = ['sigmoid'],\n",
    "          alpha = 1e-2, batch_size = 64, n_epochs = 50, opt = 'SGD', b1 = 0.9, b2 = 0.999,\n",
    "          eta0 = 0.1, eta_type = 'static', t1 = 50,\n",
    "          penality = None, lmd = 0):\n",
    "        \"\"\"Initialize the NN means:\n",
    "        - create the architecture (layers, neurons, activations functions) of the network,\n",
    "        - set all the paramaters needed for the backpropagation and the optimization algorithms.\n",
    "        - NOT initialize inputs X and output Y.\n",
    "\n",
    "        Args:\n",
    "        - hidden_layers_dims (list): list of number of neurons for each hidden (!) layer (e.g. [2,3,4,2])\n",
    "        - hidden_activations (list): activation functions of the hidden layers. If len() = 1 and number of hidden layers > 1, it automatically sets a list of the same activation functions (* n_hidden_layers).\n",
    "        - alpha (float): parameter for 'elu' and 'leaky_relu' activation function\n",
    "        - opt, batch_size, n_epochs, b1, b2: parameters to set up the optimization algorithm\n",
    "        - eta0, eta_type, t1: parameters to set up the learning rate\n",
    "        - penality, lmd: parameters to set the regularization routine\n",
    "        \"\"\"\n",
    "\n",
    "        ### Layers and neurons\n",
    "        self.n_hidden_layers = len(hidden_layers_dims)\n",
    "        self.hidden_layers_dims = hidden_layers_dims # list of n. of neurons in each hidden layer (no input nor output)\n",
    "\n",
    "        #### Hidden activation functions:\n",
    "\n",
    "        # Check if activations functions list has same lenght ad n_hidden_layers\n",
    "        assert(len(hidden_activations) == len(self.n_hidden_layers), \"Lenght of 'hidden_activations' list doesn't match with 'hidden_layer' lenght.\")\n",
    "\n",
    "        # Check if activation functions are within the list\n",
    "        if all(hidden_activations[i] not in list(activations.keys()) for i in range(len(hidden_activations))):\n",
    "          raise ValueError(\"Activation functions must be defined within \"+str(list(activations.keys())))\n",
    "        else:\n",
    "          self.activations_list = hidden_activations\n",
    "\n",
    "        #### Output activation function: (default: Regression)\n",
    "        self.activations_list.append(None)\n",
    "\n",
    "        ### Parameters\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size; self.n_epochs = n_epochs\n",
    "        self.opt = opt # Optimization algorithm ['SGD', 'ADAGRAD', 'RMS', 'ADAM', 'SGDM']\n",
    "\n",
    "        if opt not in OPTIMIZERS:\n",
    "          raise ValueError(\"Optimizer must be defined in \"+str(OPTIMIZERS))\n",
    "\n",
    "        self.b1 = b1; self.b2 = b2\n",
    "        self.eta0 = eta0; self.eta_type = eta_type; self.t1 = t1\n",
    "\n",
    "        if eta_type not in ETAS:\n",
    "          raise ValueError(\"Learning rate type must be defined within \"+str(ETAS))\n",
    "        self.penality = penality; self.lmd = lmd\n",
    "        \n",
    "########## FIT THE NETWORK\n",
    "    def fit(self, X, Y, seed = None):\n",
    "        \"\"\"Fit the network with the given input X and output Y.\n",
    "        Initialiaze layers, parameters and train the network.\"\"\"\n",
    "\n",
    "        assert(X.shape[0] == Y.shape[0], \"Size of input is different of size of the output\")\n",
    "\n",
    "        self.n_categories = Y.shape[1]\n",
    "        self.n_features = X.shape[1]\n",
    "        self.n_inputs = X.shape[0] # = Y.shape[0]\n",
    "\n",
    "        self.layers_dims = [self.n_feutures] + self.hidden_layers_dims + [self.n_categories] # [ n. of input features, n. of neurons in hidden layer-1,.., n. of neurons in hidden layer-n shape, output]\n",
    "\n",
    "        # 0) INITIALIZE PARAMATERS (WEIGHTS AND BIASES)\n",
    "        self.parameters = self.init_parameters(seed)\n",
    "\n",
    "        # 1) TRAIN THE NETWORK\n",
    "        return self.train(X, Y)\n",
    "          \n",
    "          \n",
    "########## 0) INITIALIZE PARAMATERS (WEIGHTS AND BIASES)\n",
    "    def init_parameters(self, seed):\n",
    "        \"\"\" Initialize the weights and the biases in the given achitecture (i.e. [2,4,3,2] as list of number of neurons), using a normal distribution N(0,sqrt(2/fanin)).\"\n",
    "        Args:\n",
    "            seed (int): random seed to generate weights\n",
    "        Returns:\n",
    "            parameters (dict): dictionary containing weights matrixes and biases vectors, named as \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    - Wl: weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    - bl: bias vector of shape (layer_dims[l], 1)\n",
    "        \"\"\"\n",
    "        if seed != None:\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        parameters = {}\n",
    "        L = len(self.layer_dims)\n",
    "        \n",
    "        for l in range(1, L):\n",
    "            parameters['W' + str(l)] = np.random.normal(0,np.sqrt(2.0/self.layer_dims[l-1]),(self.layer_dims[l], self.layer_dims[l-1]))\n",
    "            parameters['b' + str(l)] = np.random.normal(0,np.sqrt(2.0/self.layer_dims[l-1]),(self.layer_dims[l], 1))\n",
    "        \n",
    "        return parameters\n",
    "\n",
    "########## 1) TRAIN THE NETWORK\n",
    "    def train(self, X, Y): # i.e. SDG\n",
    "        \"\"\" The core algorithm of the NN object:\n",
    "        1) Feed Forward to arrive to the output AL\n",
    "        2) Backpropagation to evaluate all the grads for each example/feuture\n",
    "        3) Update parameters (weights and biases) according to the optimizer (throughout the examples/feutures)\n",
    "        4) All wrapped up in the STOCASTIC part (i.e. epochs and mini batches) of the SGD algorithm\n",
    "        \"\"\"\n",
    "        \n",
    "        self.grads = {}\n",
    "        self.costs = []\n",
    "        \n",
    "        idx = np.arange(0, self.n_features)\n",
    "        count=0\n",
    "        \n",
    "        for epoch in range(1,self.n_epochs+1):\n",
    "            np.random.shuffle(idx)\n",
    "            X = X[:,idx]\n",
    "            Y = Y[:,idx]\n",
    "            for i in range(0, self.n_features, self.batch_size):\n",
    "                count += 1\n",
    "                X_batch = X[:,i:i + self.batch_size]\n",
    "                Y_batch = Y[:,i:i + self.batch_size]\n",
    "  \n",
    "                # a) Forward propagation\n",
    "                AL, cache = self.forward_propagation(X_batch)\n",
    "                \n",
    "                # b) Compute cost function\n",
    "                cost = self.compute_cost(AL, Y_batch)\n",
    "                self.costs.append(cost)\n",
    "                \n",
    "                # c) Back propagation\n",
    "                grads = self.back_propagation(AL, Y_batch, cache)\n",
    "                self.grads = grads\n",
    "                            \n",
    "                # d) Update parameters (weights and biases)\n",
    "                eta = self.learning_rate(X, self.eta0, self.eta_type, t = (epoch * self.batch_size + i), self.t1)\n",
    "                self.parameters = self.update_opt_parameters(grads, eta)\n",
    "\n",
    "        return self\n",
    "        \n",
    "########## 1a) Forward propagation:\n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\"\n",
    "        Feed Forward algorithm\n",
    "        \n",
    "        Args:\n",
    "            - X (array/matrix): input data of shape (input size, number of examples/feutures)\n",
    "            \n",
    "        Returns:\n",
    "            - AL (array/matrix): output layer\n",
    "            - caches (list): 'linear_activation_forward' (A_(l-1), Wl, bl, Zl), dim = [0,L-1]\n",
    "        \"\"\"\n",
    "        Al = X\n",
    "        L = self.n_hidden_layers\n",
    "        caches = []\n",
    "        \n",
    "        for l in range(1, L): # [1,...,L-1]\n",
    "        # NB: W1 = X (A0) --(1)--> A1\n",
    "        #     WL = A(L-1) --(L-1)--> O(AL)\n",
    "        #     len(activations_list) = L-1 ([0,L-2]\n",
    "        \n",
    "            A_prev = Al\n",
    "            Wl = self.parameters['W' + str(l)]\n",
    "            bl = self.parameters['b' + str(l)]\n",
    "            Zl = np.dot(Wl, A_prev) + bl\n",
    "            \n",
    "            #Evaluate the next layer\n",
    "            act_func = ACTIVATIONS[self.activations_list[l-1]] #hidden_activation(Z, l)\n",
    "            Al = act_func(Zl, self.alpha).eval()\n",
    "            \n",
    "            # Record all the elements of evaluating each layer (A_(l-1), Wl + bl = Zl, A_(l-1) + Zl = Al ***yuppy***)\n",
    "            cache = (A_prev, Wl, bl, Zl)\n",
    "            caches.append(cache)\n",
    "\n",
    "        AL = Al # no need but it helps to read it\n",
    "        return AL, caches\n",
    "\n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError(\"Method NeuralNetwork.predict is abstract and cannot be called.\")\n",
    "\n",
    "########## 1b) Compute cost function:\n",
    "    def cost_function(self, A, Y):\n",
    "        raise NotImplementedError(\"Method NeuralNetwork.cost_function is abstract and cannot be called.\")\n",
    "              \n",
    "    def compute_cost(self, AL, Y):\n",
    "        \"\"\"\n",
    "        Implement the cost function with optional regulation tecniques 'l1' and 'l2'\n",
    "        \n",
    "        Args:\n",
    "          - AL (array/matrix): post-activation, probability vector, output of forward propagation\n",
    "          - Y (array/matrix): \"true\" labels vector, of shape (output size, number of examples)\n",
    "        \n",
    "        Returns:\n",
    "          - cost (float): value of the regularized loss function\n",
    "        \"\"\"\n",
    "\n",
    "        cost = cost_function(AL,Y)\n",
    "        \n",
    "        L = len(self.parameters)//2\n",
    "        \n",
    "        if penality == 'l2' and lmb != 0:\n",
    "            sum_weights = 0\n",
    "            for l in range(1, L):\n",
    "                sum_weights = sum_weights + np.sum(np.square(self.parameters['W' + str(l)]))\n",
    "            cost = cost + sum_weights * (lmb/(2*m))\n",
    "        elif penality == 'l1' and lmb != 0:\n",
    "            sum_weights = 0\n",
    "            for l in range(1, L):\n",
    "                sum_weights = sum_weights + np.sum(np.abs(self.parameters['W' + str(l)]))\n",
    "            cost = cost + sum_weights * (lmb/(2*m))\n",
    "    \n",
    "    return cost\n",
    "\n",
    "########## 1c) Back propagation:\n",
    "    def back_propagation(self, AL, Y, cache):\n",
    "        \"\"\"Implement the backward propagation\n",
    "    \n",
    "        Args:\n",
    "            AL (array/matrix): probability vector, output of the forward propagation\n",
    "            Y (array/matrix): true \"label\" vector (containing 0 if not the right digit, 1 if right digit)\n",
    "            \n",
    "        Returns:\n",
    "             grads (dict): gradients of A, W, b for each HIDDEN layer (no cost function yet)\n",
    "               grads[\"dAl\"] = ...\n",
    "               grads[\"dWl\"] = ...\n",
    "               grads[\"dbl\"] = ...\n",
    "        \"\"\"\n",
    "        \n",
    "        grads = {}\n",
    "        L = len(self.layer_dims) # the number of layers\n",
    "    \n",
    "        m = AL.shape[1]\n",
    "        Y = Y.reshape(AL.shape)\n",
    "    \n",
    "        # Error of the output layer (initializes the backprop alg)\n",
    "        dZL = gradient_of(cost_function, AL, Y)\n",
    "        \"\"\"#dZL = AL - Y # here derivative pf cost function!!!\n",
    "        #delO = self.sigmoid_derivative(self.output)*(self.output - targets)*sum(inputs.T)\"\"\"\n",
    "    \n",
    "        AL, W, b, Z = caches[L-1]\n",
    "        grads[\"dW\" + str(L)] = np.dot(dZL,AL.T)/m\n",
    "        grads[\"db\" + str(L)] = np.sum(dZL,axis=1,keepdims=True)/m\n",
    "        grads[\"dA\" + str(L-1)] = np.dot(W.T,dZL)\n",
    "    \n",
    "        for l in reversed(range(L-1)): # Loop from l=L-2 to l=0\n",
    "        \n",
    "            A_prev, Wl, bl, Zl = caches[l]\n",
    "            \n",
    "            m = A_prev.shape[1]\n",
    "            dA_prev = grads[\"dA\" + str(l + 1)]\n",
    "            \n",
    "            act_func = ACTIVATIONS[self.activations_list[l-1]] # not sureeeeee of l-1\n",
    "            dAl = act_func(Zl,self.alpha).grad()\n",
    "            dZl = np.multiply(dA_prev,dAl)\n",
    "            grads[\"dA\" + str(l)] = np.dot(W.T,dZl)\n",
    "            grads[\"dW\" + str(l + 1)] = np.dot(dZl,A_prev.T)/m\n",
    "            grads[\"db\" + str(l + 1)] = np.sum(dZl,axis=1,keepdims=True)/m\n",
    "            \n",
    "            if penality == 'l2':\n",
    "                grads[\"dW\" + str(l + 1)] += ((lamda * Wl)/m)\n",
    "            elif penality == 'l1':\n",
    "                grads[\"dW\" + str(l + 1)] += ((lamda * np.sign(Wl+DELTA))/m)\n",
    "\n",
    "        return grads\n",
    "        \n",
    "########## 1d) Choose the learning rate schedule\n",
    "    def learning_rate(self, X, eta0, eta_type, t, t1):\n",
    "        \"\"\"\n",
    "        Update the learning rate when it is different than a given costant value\n",
    "        Test that the algorithm converges (comparison with Hessian matrix eigenvalues)\n",
    "        \n",
    "        Args:\n",
    "          - X (array/matrix): input data, needed to evaluate the Hessian matrix\n",
    "          - eta0 (float): basic parameter for many learning rate schedule\n",
    "          - eta_type (string): name of the choosen learning rate schedule\n",
    "          - t (float): uptdating parameters to evaluate dynamic learning rate\n",
    "          - t1 (float): paramater needed for the learning rate 'schedule'\n",
    "          \n",
    "        Returns:\n",
    "          - eta (float): learning rate\n",
    "        \"\"\"\n",
    "    \n",
    "        # Evaluate the hessian metrix to test eta < max H's eigvalue\n",
    "        H = (2.0/X.shape[0])* (X.T @ X)\n",
    "        eigvalues, eigvects = np.linalg.eig(H)\n",
    "        eta_opt = 1.0/np.max(eigvalues)\n",
    "        eta = eta_opt\n",
    "        \n",
    "        if eta_type == 'static':\n",
    "            eta = eta0\n",
    "        elif eta_type == 'schedule':\n",
    "            t0 = eta0 * t1\n",
    "            eta = learning_schedule(t, t0=t0, t1=t1)\n",
    "        elif eta_type == 'invscaling':\n",
    "            power_t = 0.25 # one can change it but I dont want to overcrowd the arguments\n",
    "            eta = eta0 / pow(t, power_t)\n",
    "        elif eta_type == 'hessian':\n",
    "            pass\n",
    "            \n",
    "        assert(eta > eta_opt, \"Learning rate higher than the inverse of the max eigenvalue of the Hessian matrix: SGD will not converge to the minimum. Need to set another learning rate or its paramentes.\")\n",
    "        \n",
    "        return eta\n",
    "        \n",
    "        \n",
    "########## 1d) Update parameters (weights and biases)\n",
    "    def update_opt_parameters(self, grads,eta):\n",
    "        \"\"\"\n",
    "        Update parameters using gradient descent algorithm with different optimizers.\n",
    "        OBS: The \"Stochastic\" is in the external loop\n",
    "    \n",
    "        Args:\n",
    "        - grads (dict): output gradients from BP\n",
    "    \n",
    "        Returns:\n",
    "        - parameters (dict): updated parameters\n",
    "        \"\"\"\n",
    "        \n",
    "        # To make it easier to go through the code:\n",
    "        parameters = self.parameters\n",
    "        b1 = self.b1; b2 = self.b2\n",
    "        \n",
    "        L = len(parameters) // 2 # number of layers in the neural network\n",
    "                \n",
    "      # Initialize optimization paramaters:\n",
    "      for l in range(1, len(self.layer_dims)):\n",
    "            opt_parameters['vdw' + str(l)] = np.zeros((self.layer_dims[l], self.layer_dims[l-1]))\n",
    "            opt_parameters['vdb' + str(l)] = np.zeros((self.layer_dims[l], 1))\n",
    "            opt_parameters['sdw' + str(l)] = np.zeros((self.layer_dims[l], self.layer_dims[l-1]))\n",
    "            opt_parameters['sdb' + str(l)] = np.zeros((self.layer_dims[l], 1))\n",
    "            \n",
    "        for l in range(L):\n",
    "            # Just rewrite to make the code easier to follow\n",
    "            W = parameters[\"W\" + str(l+1)]\n",
    "            dW = grads[\"dW\" + str(l + 1)]\n",
    "            b = parameters[\"b\" + str(l+1)]\n",
    "            db = grads[\"db\" + str(l + 1)]\n",
    "            \n",
    "            if method == 'SGD':\n",
    "                W -= eta * dW\n",
    "                b -= eta * db\n",
    "                \n",
    "            else:\n",
    "                vdw = opt_parameters['vdw'+str(l+1)]\n",
    "                vdb = opt_parameters['vdb'+str(l+1)]\n",
    "                sdw = opt_parameters['sdw'+str(l+1)]\n",
    "                sdb = opt_parameters['sdb'+str(l+1)]\n",
    "                \n",
    "                if method == 'SGDM':\n",
    "                \n",
    "                    vdb = b1 * vdb - eta * db\n",
    "                    vdw = b1 * vdw - eta * dW\n",
    "                    \n",
    "                    W += vdw\n",
    "                    b += vdb\n",
    "                    \n",
    "                elif method == 'ADAGRAD':\n",
    "                \n",
    "                    sdb = sbd + np.square(db)\n",
    "                    sdw = sdw + np.square(dW)\n",
    "                    \n",
    "                    W -= eta * (dW/(np.sqrt(sdw) + DELTA))\n",
    "                    b -= eta *(db/(np.sqrt(sdb) + DELTA))\n",
    "                    \n",
    "                elif method == 'RMS':\n",
    "                \n",
    "                    sdb = b1 * sbd + (1. - b1) * np.square(db)\n",
    "                    sdw = b1 * sdw + (1. - b1) * np.square(dW)\n",
    "                    \n",
    "                    W -= eta * (dW/(np.sqrt(sdw) + DELTA))\n",
    "                    b -= eta *(db/(np.sqrt(sdb) + DELTA))\n",
    "                    \n",
    "                elif method == 'ADAM':\n",
    "                    vdb = (b1 * vdb + (1.-b1) * db)/(1.-b1)\n",
    "                    vdw = (b1 * vdw + (1.-b1) * dW)/(1.-b1)\n",
    "                    sdb = (b2 * sdb + (1.-b2) * np.square(db))/(1.-b2)\n",
    "                    sdw = (b2 * sdw + (1.-b2) * np.square(dW))/(1.-b2)\n",
    "                    \n",
    "                    W -= eta * (vdw/(np.sqrt(sdw) + DELTA))\n",
    "                    b -= eta * (vdb/(np.sqrt(sdb) + DELTA))\n",
    "                    \n",
    "          # Update back the parameters in the dictionaries\n",
    "                opt_parameters['vdw'+str(l+1)] = vdw\n",
    "                opt_parameters['vdb'+str(l+1)] = vdb\n",
    "                opt_parameters['sdw'+str(l+1)] = sdw\n",
    "                opt_parameters['sdb'+str(l+1)] = sdb\n",
    "                                                          \n",
    "            parameters[\"W\" + str(l+1)] = Wl\n",
    "            grads[\"dW\" + str(l + 1)] = dWl\n",
    "            parameters[\"b\" + str(l+1)] = bl\n",
    "            grads[\"db\" + str(l + 1)] = dbl\n",
    "            \n",
    "        self.parameters = parameters\n",
    "\n",
    "        return parameters\n",
    "        \n",
    "\n",
    "##################### NN_Regression #############################################\n",
    "class NN_Regression(NeuralNetwork):\n",
    "    \"\"\"Derivative class of NeuralNetwork that implements methods for Linear Regression:\n",
    "        - output activation function = None (default)\n",
    "        - cost function = MSE\n",
    "        - predict = returns function prediction\n",
    "    \"\"\"\n",
    "    def cost_function(A, Y): #MSE\n",
    "          return mean_squared_error(A,Y)\n",
    "          \n",
    "    def predict(self, X): #check!!!\n",
    "        ''' Predicting values with FF (with no parameters optimization)\n",
    "\n",
    "        Args: X (array/matrix): input data of shape (input size, number of examples/feutures)\n",
    "        Returns: Y (array/matrix): output values\n",
    "        '''\n",
    "        \n",
    "        out, _ = self.forward_propagation(X)\n",
    "        return out.T\n",
    "          \n",
    "##################### NN_Classifier #############################################\n",
    "class NN_Classifier(NeuralNetwork):\n",
    "    \"\"\"Derivative class of NeuralNetwork that implements methods for Logistic Regression:\n",
    "        - output activation function = 'softmax'\n",
    "        - cost function = Cross-Entropy\n",
    "        - predict = can return the probability of occurance or the index (i.e. digit) of the maximum probability in n_categories (Y columns).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__(X)\n",
    "        self.activations_list[-1] = 'softmax'\n",
    "        \n",
    "    def cost_function(A, Y): # Cross_Entropy\n",
    "          return np.squeeze(-np.sum(np.multiply(np.log(A),Y))/Y.shape[1])\n",
    "          \n",
    "    def predict(self, X, proba=False): #check!!!\n",
    "        ''' Predicting values (with no parameters optimization)\n",
    "\n",
    "        Args:\n",
    "            - X (array/matrix): input data of shape (input size, number of examples/feutures)\n",
    "            - prob (bool): True - return function values\n",
    "                           False – return filtered best likelihood\n",
    "            \n",
    "        Returns:\n",
    "            - Y (array/matrix): output values, as probability values or index (i.e. digit) of n_categories (Y columns).\n",
    "        '''\n",
    "        \n",
    "        out, _ = self.forward_propagation(X,self.hidden_layers,self.parameters,self.keep_proba,self.seed)\n",
    "        if proba == True:\n",
    "            return out.T\n",
    "        else:\n",
    "            # Obtain prediction by taking the class with the highest likelihood\n",
    "            return np.argmax(out, axis=0) #check!!!\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def hidden_activation(Z, l):\n",
    "        Condense if-conditions of the activation functions.\n",
    "        i =l - 1\n",
    "        if self.activations[i] == 'sigmoid':\n",
    "            return sigmoid(Z)\n",
    "        elif self.activations[i] == 'tanh':\n",
    "            return tanh(Z)\n",
    "        elif self.activations[i] == 'relu':\n",
    "            return relu(Z)\n",
    "        elif self.activations[i] == 'leaky_relu':\n",
    "            return leaky_relu(Z)\n",
    "        elif self.activations[i] == 'elu':\n",
    "            return elu(Z)\n",
    "        elif self.activations[i] == 'softmax':\n",
    "            return softmax(Z)\n",
    "            \n",
    "  activations = {'sigmoid': sigmoid, 'tanh': tanh, 'relu': relu, 'leaky_relu': leaky_relu, 'elu': elu, 'softmax': softmax}\n",
    "        \"\"\"\n",
    "    \"\"\"\n",
    "                1) Same hidden activation function approach:\n",
    "            - hidden_activation (string): activation function of the hidden layers (default: sigmoid)\n",
    "            - output_activation (string): activation function of the output layer. If None, it means that we are dealing with Linear Regression, otherwise it is Logistic Regression (recommended 'softmax').\n",
    "            \n",
    "            2) Different hidden activation function approach:\n",
    "            - activations (list): list of activations functions (hidden and output)\n",
    "            \n",
    "                      if activations = None:\n",
    "              if hidden_activation not in list(activations.keys()):\n",
    "                  raise ValueError(\"Hidden activation function must be defined within \"+str(list(activations.keys())))\n",
    "              #else:self.hidden_activation = hidden_activation\n",
    "                  \n",
    "              if output_activation not in list(activations.keys()):\n",
    "                  raise ValueError(\"Output activation function must be defined within \"+str(list(activations.keys())))\n",
    "              #else:self.output_activation = output_activation\n",
    "                  \n",
    "              self.activations_list = [hidden_activation] * self.n_hidden_layers\n",
    "              self.activations_list[-1] = output_activation\n",
    "              \n",
    "          else: # to test\n",
    "              assert(len(activations) == (len(self.n_hidden_layers)+1), \"Lenght of 'activations' list doesn't match with 'hidden_layer' lenght (+1).\"\n",
    "              if activations[:] not in list(activations.keys()):\n",
    "                  raise ValueError(\"Activation functions must be defined within \"+str(list(activations.keys())))\n",
    "              self.activations_list = activations\n",
    "              \n",
    "              \n",
    "                        # Output activation function:\n",
    "          if regression not in ['Linear, Logistic']:\n",
    "              raise ValueError(\"Regression must be defined as 'Linear' or 'Logistic'.\")\n",
    "          if regression == 'Linear':\n",
    "              self.activations_list.append(None)\n",
    "          elif regression == 'Logistic':\n",
    "              self.activations_list.append('softmax')\n",
    "            \"\"\"\n",
    "\"\"\"\n",
    "      #hidden_activation = 'sigmoid', output_activation=None, activations = None,\n",
    "      \n",
    "              \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
